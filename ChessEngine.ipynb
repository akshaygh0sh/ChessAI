{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import ChessGame\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from gym import spaces\r\n",
    "from pettingzoo import AECEnv\r\n",
    "from pettingzoo.utils import agent_selector\r\n",
    "\r\n",
    "from pettingzoo.test import api_test\r\n",
    "from pettingzoo.test import performance_benchmark\r\n",
    "\r\n",
    "import ray\r\n",
    "import torch\r\n",
    "\r\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\r\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\r\n",
    "from ray.rllib.utils.torch_ops import FLOAT_MIN, FLOAT_MAX\r\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\r\n",
    "\r\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\r\n",
    "# from ray.rllib.env import BaseEnv\r\n",
    "# from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\r\n",
    "# from ray.rllib.policy import Policy\r\n",
    "# from ray.rllib.policy.sample_batch import SampleBatch\r\n",
    "\r\n",
    "tf1, tf, tfv = try_import_tf()\r\n",
    "torch, nn = try_import_torch()\r\n",
    "\r\n",
    "class ChessEnv (AECEnv):\r\n",
    "    metadata = {'render.modes': ['human']}\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # White is represented as player_0 and black is represented as player_1\r\n",
    "        self.agents = [\"player_\" + str(num) for num in range (2)]\r\n",
    "        self.possible_agents = [\"player_\" + str(num) for num in range (2)]\r\n",
    "        self.agent_name_mapping = { \"player_0\" : 0,\r\n",
    "                                    \"player_1\" : 1}\r\n",
    "        self._agent_selector = agent_selector(self.agents)\r\n",
    "\r\n",
    "        self.action_spaces = {name: spaces.Discrete(4672) for name in self.possible_agents}\r\n",
    "        self.observation_spaces = {name: spaces.Dict({\r\n",
    "            'observation': spaces.Box(low=-1, high=500, shape=(5, 8, 8, 14), dtype=np.float32),\r\n",
    "            'action_mask': spaces.Box(low=0, high=1, shape=(4672,), dtype=np.float32)\r\n",
    "        }) for name in self.possible_agents}\r\n",
    "\r\n",
    "        self.blackEval = 0.5\r\n",
    "        self.whiteEval = 0.5\r\n",
    "        self.board = ChessGame.Board()\r\n",
    "        self.rewards = None\r\n",
    "        self.dones = None\r\n",
    "        self.infos = {name: {} for name in self.agents}\r\n",
    "\r\n",
    "        self.agent_selection = None\r\n",
    "\r\n",
    "        \r\n",
    "    def observe(self, agent):\r\n",
    "        observation = []\r\n",
    "        action_mask = np.zeros(shape = 4672, dtype = np.int16)\r\n",
    "\r\n",
    "        if (self.board.whiteToMove):\r\n",
    "            observation = self.board.GetBoard()[0]\r\n",
    "            if (agent == self.possible_agents[0]):\r\n",
    "                for moves in self.board.allLegalMoves:\r\n",
    "                    action_mask[moves] = 1\r\n",
    "        else:\r\n",
    "            observation = self.board.GetBoard()[1]\r\n",
    "            if (agent == self.possible_agents[1]):\r\n",
    "                for moves in self.board.allLegalMoves:\r\n",
    "                    action_mask[moves] = 1\r\n",
    "        return {\"observation\" : observation, \"action_mask\" : action_mask}\r\n",
    "\r\n",
    "    def reset (self):\r\n",
    "        self.board.ResetBoard()\r\n",
    "\r\n",
    "        self.agents = self.possible_agents[:]\r\n",
    "        self.rewards = {agent : 0 for agent in self.agents}\r\n",
    "        self._cumulative_rewards =  {agent : 0 for agent in self.agents}\r\n",
    "        self.dones = {agent: False for agent in self.agents}\r\n",
    "        self.observation = {agent : None for agent in self.agents}\r\n",
    "        self.infos = {agent: {} for agent in self.agents}\r\n",
    "\r\n",
    "        self._agent_selector = agent_selector(self.agents)\r\n",
    "        self.agent_selection = self._agent_selector.reset()\r\n",
    "        \r\n",
    "\r\n",
    "    def step (self, action):\r\n",
    "        if self.dones[self.agent_selection]:\r\n",
    "            return self._was_done_step(action)\r\n",
    "        \r\n",
    "        self.rewards = {agent : 0 for agent in self.agents}\r\n",
    "        current_agent = self.agent_selection\r\n",
    "        self._cumulative_rewards[current_agent] = 0\r\n",
    "\r\n",
    "        # Make the move\r\n",
    "        self.board.Move(action)\r\n",
    "\r\n",
    "        if (self.board.whiteToMove):\r\n",
    "            # If it's white to move and there's checkmate on the board, white lost (and black won); apply appropriate rewards\r\n",
    "            if (self.board.gameState > 0):\r\n",
    "                self.rewards[self.agents[0]] = -1\r\n",
    "                self.rewards[self.agents[1]] = 1\r\n",
    "            # Game is drawn somehow, each agent gets a reward of 0\r\n",
    "            elif (self.board.gameState < 0):\r\n",
    "                self.rewards[self.agents[0]] = self.rewards[self.agents[1]] = 0\r\n",
    "            else:\r\n",
    "                self.rewards[self.agents[0]] = (self.board.eval - self.whiteEval) * 5\r\n",
    "                self.whiteEval = self.board.eval\r\n",
    "\r\n",
    "        else:\r\n",
    "            # If it's black to move and there's checkmate on the board, black lost (and white won); apply appropriate rewards\r\n",
    "            if (self.board.gameState > 0):\r\n",
    "                self.rewards[self.agents[0]] = 1\r\n",
    "                self.rewards[self.agents[1]] = -1\r\n",
    "            # Game is drawn somehow, each agent gets a reward of 0\r\n",
    "            elif (self.board.gameState < 0):\r\n",
    "                self.rewards[self.agents[0]] = self.rewards[self.agents[1]] = 0\r\n",
    "            else:\r\n",
    "                self.rewards[self.agents[1]] = (self.board.eval - self.blackEval) * 5\r\n",
    "                self.blackEval = self.board.eval\r\n",
    "\r\n",
    "        if (self.board.gameState != 0):\r\n",
    "            for name in self.agents:\r\n",
    "                self.infos[name] = self.board.listOfMoves\r\n",
    "                self.dones[name] = True\r\n",
    "\r\n",
    "        self.agent_selection = self._agent_selector.next()\r\n",
    "        self._accumulate_rewards()\r\n",
    " \r\n",
    "    def render (self):\r\n",
    "        self.board.ShowBoard()\r\n",
    "    \r\n",
    "    def close(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "# Formal PettingZoo API Test & Performance Benchmarks\r\n",
    "env = ChessEnv()\r\n",
    "# env = wrappers.BaseWrapper(env)\r\n",
    "# env = wrappers.OrderEnforcingWrapper(env)\r\n",
    "api_test(env, num_cycles = 50, verbose_progress = True)\r\n",
    "performance_benchmark(env)\r\n",
    "\r\n",
    "class ChessNetwork (TorchModelV2, nn.Module):\r\n",
    "    def __init__ (self, obs_space, action_space, num_outputs, model_config, name, **kwargs):\r\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name, **kwargs)\r\n",
    "        nn.Module.__init__(self)\r\n",
    "\r\n",
    "        action_embed_size = 4672\r\n",
    "        self.action_embed_model = FullyConnectedNetwork(\r\n",
    "            spaces.Box(low=-1, high=500, shape=(5, 8, 8, 14)), action_space, action_embed_size,\r\n",
    "            model_config, name + \"action_embed\")\r\n",
    "\r\n",
    "    def forward(self, input_dict, state, seq_lens):\r\n",
    "        # Extract the available actions tensor from the observation.\r\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\r\n",
    "\r\n",
    "        # Compute the predicted action embedding\r\n",
    "        action_logits, _ = self.action_embed_model({\r\n",
    "            \"obs\": input_dict[\"obs\"]['observation']\r\n",
    "        })\r\n",
    "        \r\n",
    "        # Masks out invalid actions\r\n",
    "        inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX)\r\n",
    "\r\n",
    "        return action_logits + inf_mask, state\r\n",
    "\r\n",
    "    def value_function(self):\r\n",
    "        return self.action_embed_model.value_function()\r\n",
    "\r\n",
    "class AlgebraicNotation (DefaultCallbacks):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # Counter variable of number of total episodes\r\n",
    "        self.episode_num = 1\r\n",
    "        # Log chess games (algebraic notation) every \"log_freq\" games\r\n",
    "        self.log_freq = 25\r\n",
    "    def on_episode_end (self, *, worker, base_env, episode, env_index, **kwargs):\r\n",
    "        if (self.episode_num % self.log_freq == 0):\r\n",
    "            board = base_env.get_unwrapped()[0].env.board\r\n",
    "            gameFile = open(\"C:/Users/408aa/Desktop/Code/Python/Chess_AI/AlgebraicNotations.txt\", \"a\")\r\n",
    "            algNotation = str()\r\n",
    "\r\n",
    "            for turn in board.listOfMoves:\r\n",
    "                algNotation = \" \".join([algNotation, turn])\r\n",
    "            \r\n",
    "            algNotation = \"\".join([\"Game #\", str(self.episode_num), \": \", algNotation, \"\\n\", \"\\n\"])\r\n",
    "            # print (algNotation)\r\n",
    "            gameFile.write(algNotation)\r\n",
    "            gameFile.close()\r\n",
    "        self.episode_num +=1\r\n",
    "\r\n",
    "def env_creator():\r\n",
    "    env = ChessEnv()\r\n",
    "    return env\r\n",
    "\r\n",
    "# Clear the file with the game logs\r\n",
    "def clearLogFile():\r\n",
    "    gameFile = open(\"C:/Users/408aa/Desktop/Code/Python/Chess_AI/AlgebraicNotations.txt\", \"w\")\r\n",
    "    gameFile.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from ray.rllib.agents import ppo\r\n",
    "from ray.rllib.env import PettingZooEnv\r\n",
    "from ray import tune\r\n",
    "\r\n",
    "from ray.rllib.models import ModelCatalog\r\n",
    "from ray.tune.registry import register_env\r\n",
    "# from ray.rllib.agents.registry import get_trainer_class\r\n",
    "\r\n",
    "# from ray.tune.logger import pretty_print\r\n",
    "\r\n",
    "import os\r\n",
    "\r\n",
    "\r\n",
    "# Register Model and Environment\r\n",
    "ModelCatalog.register_custom_model(\"ChessNetwork\", ChessNetwork)\r\n",
    "\r\n",
    "register_env(\"ChessEnv\", lambda config : PettingZooEnv(env_creator()))\r\n",
    "\r\n",
    "test_env = PettingZooEnv(env_creator())\r\n",
    "obs_space = test_env.observation_space\r\n",
    "act_space = test_env.action_space\r\n",
    "\r\n",
    "config = ppo.DEFAULT_CONFIG.copy()\r\n",
    "\r\n",
    "config[\"multiagent\"] = {\r\n",
    "    \"policies\": {\r\n",
    "        \"player_0\": (None, obs_space, act_space, {}),\r\n",
    "        \"player_1\": (None, obs_space, act_space, {}),\r\n",
    "    },\r\n",
    "    \"policy_mapping_fn\": lambda agent_id: agent_id\r\n",
    "}\r\n",
    "\r\n",
    "config[\"num_workers\"] = 5\r\n",
    "config[\"num_envs_per_worker\"] = 1\r\n",
    "config[\"num_cpus_per_worker\"] = 2\r\n",
    "config[\"num_cpus_for_driver\"] = 4\r\n",
    "# config[\"num_gpus\"] = 1\r\n",
    "config[\"framework\"] = \"torch\"\r\n",
    "config[\"model\"] = {\r\n",
    "    \"custom_model\" : \"ChessNetwork\",\r\n",
    "}\r\n",
    "config[\"env\"] = \"ChessEnv\"\r\n",
    "config[\"horizon\"] = 150\r\n",
    "config[\"rollout_fragment_length\"] = 500\r\n",
    "config[\"callbacks\"] = AlgebraicNotation\r\n",
    "config[\"log_level\"] = \"INFO\"\r\n",
    "ray.init(num_cpus = 16, num_gpus = 1, ignore_reinit_error = True)\r\n",
    "\r\n",
    "tune.run(\r\n",
    "    \"PPO\",\r\n",
    "    name=\"Chess_Policy\",\r\n",
    "    stop={\"timesteps_total\": 750000},\r\n",
    "    checkpoint_freq=1000,\r\n",
    "    checkpoint_at_end = True,\r\n",
    "    config=config,\r\n",
    "    local_dir = os.getcwd()\r\n",
    ")\r\n",
    "\r\n",
    "ray.shutdown()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-26 01:23:55,684\tINFO services.py:1274 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-26 01:24:07,450\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:13,071\tWARNING ppo.py:143 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=5 num_envs_per_worker=1 rollout_fragment_length=500)! Auto-adjusting `rollout_fragment_length` to 800.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:22,558\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m 2021-07-26 01:24:22,681\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:22,748\tINFO torch_policy.py:137 -- TorchPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m 2021-07-26 01:24:22,769\tINFO torch_policy.py:137 -- TorchPolicy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:22,983\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m 2021-07-26 01:24:22,983\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m 2021-07-26 01:24:22,983\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:23,055\tINFO torch_policy.py:137 -- TorchPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m 2021-07-26 01:24:23,054\tINFO torch_policy.py:137 -- TorchPolicy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m 2021-07-26 01:24:23,062\tINFO torch_policy.py:137 -- TorchPolicy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,245\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m 2021-07-26 01:24:23,237\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,308\tINFO torch_policy.py:137 -- TorchPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m 2021-07-26 01:24:23,300\tINFO torch_policy.py:137 -- TorchPolicy (worker=2) running on CPU.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,530\tINFO torch_policy.py:137 -- TorchPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,683\tINFO rollout_worker.py:1199 -- Built policy map: {'player_0': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x0000026D173F86C8>, 'player_1': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x0000026D1890B688>}\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,683\tINFO rollout_worker.py:1200 -- Built preprocessor map: {'player_0': <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x0000026D1744ABC8>, 'player_1': <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x0000026D17477EC8>}\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,683\tINFO rollout_worker.py:583 -- Built filter map: {'player_0': <ray.rllib.utils.filter.NoFilter object at 0x0000026D17422B08>, 'player_1': <ray.rllib.utils.filter.NoFilter object at 0x0000026D17462748>}\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,707\tINFO trainable.py:104 -- Trainable.setup took 10.674 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:23,709\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m 2021-07-26 01:24:23,659\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m 2021-07-26 01:24:23,729\tINFO torch_policy.py:137 -- TorchPolicy (worker=5) running on CPU.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m 2021-07-26 01:24:23,934\tINFO torch_policy.py:137 -- TorchPolicy (worker=5) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m 2021-07-26 01:24:24,185\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m 2021-07-26 01:24:24,245\tINFO torch_policy.py:137 -- TorchPolicy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m 2021-07-26 01:24:24,389\tINFO catalog.py:414 -- Wrapping <class '__main__.ChessNetwork'> as None\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m 2021-07-26 01:24:24,448\tINFO torch_policy.py:137 -- TorchPolicy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:24,599\tINFO rollout_worker.py:724 -- Generating sample batch of size 800\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:24,599\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'player_0': { 'action_mask': np.ndarray((4672,), dtype=int16, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                      'observation': np.ndarray((5, 8, 8, 14), dtype=float32, min=-1.0, max=1.0, mean=0.007)}}}\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:24,599\tINFO sampler.py:592 -- Info return from env: {0: {'player_0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:24,599\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((9152,), dtype=float32, min=-1.0, max=1.0, mean=0.006)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:24,599\tINFO sampler.py:817 -- Filtered obs: np.ndarray((9152,), dtype=float32, min=-1.0, max=1.0, mean=0.006)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:24,599\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m { 'player_0': [ { 'data': { 'agent_id': 'player_0',\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                             'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                             'info': {},\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                             'obs': np.ndarray((9152,), dtype=float32, min=-1.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                             'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                             'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                             'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                   'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:24,615\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m { 'player_0': ( np.ndarray((1,), dtype=int64, min=3869.0, max=3869.0, mean=3869.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 [],\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 { 'action_dist_inputs': np.ndarray((1, 4672), dtype=float32, min=-3.3999999521443642e+38, max=0.004, mean=-inf),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                   'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                   'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                   'vf_preds': np.ndarray((1,), dtype=float32, min=-0.001, max=-0.001, mean=-0.001)})}\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:25,537\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m { 'player_0': { 'action_dist_inputs': np.ndarray((85, 4672), dtype=float32, min=-3.3999999521443642e+38, max=0.017, mean=-inf),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'action_logp': np.ndarray((85,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'actions': np.ndarray((85,), dtype=int64, min=181.0, max=4599.0, mean=3175.047),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'advantages': np.ndarray((85,), dtype=float32, min=-0.51, max=0.349, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'agent_index': np.ndarray((85,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'dones': np.ndarray((85,), dtype=bool, min=0.0, max=1.0, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'eps_id': np.ndarray((85,), dtype=int32, min=1933257673.0, max=1933257673.0, mean=1933257673.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'infos': np.ndarray((85,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'new_obs': np.ndarray((85, 9152), dtype=float32, min=-1.0, max=86.0, mean=1.505),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'obs': np.ndarray((85, 9152), dtype=float32, min=-1.0, max=85.0, mean=1.47),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'rewards': np.ndarray((85,), dtype=float32, min=-0.389, max=0.361, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'unroll_id': np.ndarray((85,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'value_targets': np.ndarray((85,), dtype=float32, min=-0.52, max=0.339, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'vf_preds': np.ndarray((85,), dtype=float32, min=-0.01, max=-0.001, mean=-0.009)},\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   'player_1': { 'action_dist_inputs': np.ndarray((84, 4672), dtype=float32, min=-3.3999999521443642e+38, max=0.021, mean=-inf),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'action_logp': np.ndarray((84,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'actions': np.ndarray((84,), dtype=int64, min=401.0, max=4661.0, mean=3203.833),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'advantages': np.ndarray((84,), dtype=float32, min=-0.346, max=0.517, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'agent_index': np.ndarray((84,), dtype=int32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'dones': np.ndarray((84,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'eps_id': np.ndarray((84,), dtype=int32, min=1933257673.0, max=1933257673.0, mean=1933257673.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'infos': np.ndarray((84,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'new_obs': np.ndarray((84, 9152), dtype=float32, min=-1.0, max=85.0, mean=1.516),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'obs': np.ndarray((84, 9152), dtype=float32, min=-1.0, max=84.0, mean=1.481),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'rewards': np.ndarray((84,), dtype=float32, min=-0.336, max=0.409, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'unroll_id': np.ndarray((84,), dtype=int32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'value_targets': np.ndarray((84,), dtype=float32, min=-0.351, max=0.512, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                 'vf_preds': np.ndarray((84,), dtype=float32, min=-0.008, max=-0.005, mean=-0.006)}}\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m 2021-07-26 01:24:29,212\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   'policy_batches': { 'player_0': { 'action_dist_inputs': np.ndarray((400, 4672), dtype=float32, min=-3.3999999521443642e+38, max=0.022, mean=-inf),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'action_logp': np.ndarray((400,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'actions': np.ndarray((400,), dtype=int64, min=95.0, max=4641.0, mean=3131.79),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'advantages': np.ndarray((400,), dtype=float32, min=-1.665, max=2.331, mean=-0.057),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'agent_index': np.ndarray((400,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'dones': np.ndarray((400,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'eps_id': np.ndarray((400,), dtype=int32, min=164693463.0, max=1933257673.0, mean=1018606991.575),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'infos': np.ndarray((400,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'new_obs': np.ndarray((400, 9152), dtype=float32, min=-1.0, max=86.0, mean=1.438),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'obs': np.ndarray((400, 9152), dtype=float32, min=-1.0, max=85.0, mean=1.403),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'rewards': np.ndarray((400,), dtype=float32, min=-0.974, max=2.111, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'unroll_id': np.ndarray((400,), dtype=int32, min=0.0, max=8.0, mean=3.75),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'value_targets': np.ndarray((400,), dtype=float32, min=-1.675, max=2.33, mean=-0.066),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'vf_preds': np.ndarray((400,), dtype=float32, min=-0.011, max=-0.001, mean=-0.009)},\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                       'player_1': { 'action_dist_inputs': np.ndarray((395, 4672), dtype=float32, min=-3.3999999521443642e+38, max=0.022, mean=-inf),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'action_logp': np.ndarray((395,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'actions': np.ndarray((395,), dtype=int64, min=15.0, max=4661.0, mean=3188.668),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'advantages': np.ndarray((395,), dtype=float32, min=-0.914, max=1.666, mean=0.094),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'agent_index': np.ndarray((395,), dtype=int32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'dones': np.ndarray((395,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'eps_id': np.ndarray((395,), dtype=int32, min=164693463.0, max=1933257673.0, mean=1018035861.828),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'infos': np.ndarray((395,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'new_obs': np.ndarray((395, 9152), dtype=float32, min=-1.0, max=85.0, mean=1.449),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'obs': np.ndarray((395, 9152), dtype=float32, min=-1.0, max=84.0, mean=1.414),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'rewards': np.ndarray((395,), dtype=float32, min=-0.888, max=0.963, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'unroll_id': np.ndarray((395,), dtype=int32, min=1.0, max=9.0, mean=4.747),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'value_targets': np.ndarray((395,), dtype=float32, min=-0.919, max=1.661, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m                                     'vf_preds': np.ndarray((395,), dtype=float32, min=-0.009, max=-0.005, mean=-0.006)}},\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m 2021-07-26 01:24:29,482\tINFO rollout_worker.py:903 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m   'policy_batches': { 'player_0': { 'action_dist_inputs': np.ndarray((128, 4672), dtype=float32, min=-3.3999999521443642e+38, max=0.022, mean=-inf),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'action_logp': np.ndarray((128,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'actions': np.ndarray((128,), dtype=int64, min=337.0, max=4642.0, mean=3149.289),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'advantages': np.ndarray((128,), dtype=float32, min=-3.407, max=1.941, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'agent_index': np.ndarray((128,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'dones': np.ndarray((128,), dtype=bool, min=0.0, max=1.0, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'eps_id': np.ndarray((128,), dtype=int32, min=1802028.0, max=1963349134.0, mean=864029699.008),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'infos': np.ndarray((128,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'new_obs': np.ndarray((128, 9152), dtype=float32, min=-1.0, max=86.0, mean=1.56),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'obs': np.ndarray((128, 9152), dtype=float32, min=-1.0, max=85.0, mean=1.525),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'rewards': np.ndarray((128,), dtype=float32, min=-0.915, max=0.785, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'unroll_id': np.ndarray((128,), dtype=int32, min=0.0, max=8.0, mean=3.531),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'value_targets': np.ndarray((128,), dtype=float32, min=-2.204, max=1.282, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m                                     'vf_preds': np.ndarray((128,), dtype=float32, min=-0.011, max=-0.001, mean=-0.01)}},\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=5700)\u001b[0m \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 3975\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_01-30-50\n",
      "  done: false\n",
      "  episode_len_mean: 170.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.16015395522117615\n",
      "  episode_reward_mean: 0.006650244817137718\n",
      "  episode_reward_min: -0.05490392446517944\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.180171713232994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024934864486567676\n",
      "          policy_loss: 0.22749220672994852\n",
      "          total_loss: 0.29122585244476795\n",
      "          vf_explained_var: 0.8705475330352783\n",
      "          vf_loss: 0.05874667107127607\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.2271823436021805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03429098369088024\n",
      "          policy_loss: 0.2112923599779606\n",
      "          total_loss: 0.2695800270885229\n",
      "          vf_explained_var: 0.8816171884536743\n",
      "          vf_loss: 0.051429472863674164\n",
      "    num_agent_steps_sampled: 3975\n",
      "    num_agent_steps_trained: 3975\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.254395604395604\n",
      "    ram_util_percent: 66.93076923076924\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.858126424252987\n",
      "    player_1: 2.8505413234233856\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0975817684084177\n",
      "    player_1: 0.10423201322555542\n",
      "  policy_reward_min:\n",
      "    player_0: -2.849959023296833\n",
      "    player_1: -2.7942992746829987\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07947476466794437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0507993186160125\n",
      "    mean_inference_ms: 4.22341421748815\n",
      "    mean_raw_obs_processing_ms: 0.27585863025298585\n",
      "  time_since_restore: 386.99105167388916\n",
      "  time_this_iter_s: 386.99105167388916\n",
      "  time_total_s: 386.99105167388916\n",
      "  timers:\n",
      "    learn_throughput: 10.492\n",
      "    learn_time_ms: 381247.681\n",
      "    sample_throughput: 701.953\n",
      "    sample_time_ms: 5698.388\n",
      "    update_time_ms: 23.207\n",
      "  timestamp: 1627288250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         386.991</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">0.00665024</td><td style=\"text-align: right;\">            0.160154</td><td style=\"text-align: right;\">          -0.0549039</td><td style=\"text-align: right;\">               170</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 7952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_01-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 165.04347826086956\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5576299130916595\n",
      "  episode_reward_mean: 0.0003363152348395923\n",
      "  episode_reward_min: -0.5481317266821861\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 46\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.086345463991165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015545258182100952\n",
      "          policy_loss: 0.2350367633625865\n",
      "          total_loss: 0.3510952116921544\n",
      "          vf_explained_var: 0.8630298376083374\n",
      "          vf_loss: 0.11139487684704363\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.1670330613851547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01954252674477175\n",
      "          policy_loss: 0.22498223930597305\n",
      "          total_loss: 0.3130734246224165\n",
      "          vf_explained_var: 0.8868327140808105\n",
      "          vf_loss: 0.08222843427211046\n",
      "    num_agent_steps_sampled: 7952\n",
      "    num_agent_steps_trained: 7952\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.613754646840146\n",
      "    ram_util_percent: 61.714312267657995\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.5467444732785225\n",
      "    player_1: 4.0564481019973755\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.06783423669960188\n",
      "    player_1: -0.06749792146476229\n",
      "  policy_reward_min:\n",
      "    player_0: -4.067953512072563\n",
      "    player_1: -4.094876199960709\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08064383694079809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0598717663676618\n",
      "    mean_inference_ms: 4.1309570094155585\n",
      "    mean_raw_obs_processing_ms: 0.27668038511133125\n",
      "  time_since_restore: 769.0255851745605\n",
      "  time_this_iter_s: 382.0345335006714\n",
      "  time_total_s: 769.0255851745605\n",
      "  timers:\n",
      "    learn_throughput: 10.546\n",
      "    learn_time_ms: 379291.485\n",
      "    sample_throughput: 774.565\n",
      "    sample_time_ms: 5164.192\n",
      "    update_time_ms: 23.205\n",
      "  timestamp: 1627288632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         769.026</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">0.000336315</td><td style=\"text-align: right;\">             0.55763</td><td style=\"text-align: right;\">           -0.548132</td><td style=\"text-align: right;\">           165.043</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 11928\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_01-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 166.74285714285713\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5576299130916595\n",
      "  episode_reward_mean: -0.0016624246324811662\n",
      "  episode_reward_min: -0.5481317266821861\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 70\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.1155604273080826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021649044298101217\n",
      "          policy_loss: 0.2333999676629901\n",
      "          total_loss: 0.3178369989618659\n",
      "          vf_explained_var: 0.8259205222129822\n",
      "          vf_loss: 0.0779423164203763\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000004\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.095153048634529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024689261568710208\n",
      "          policy_loss: 0.22913136379793286\n",
      "          total_loss: 0.2998537067323923\n",
      "          vf_explained_var: 0.8688908815383911\n",
      "          vf_loss: 0.06331557431258261\n",
      "    num_agent_steps_sampled: 11928\n",
      "    num_agent_steps_trained: 11928\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.738533834586468\n",
      "    ram_util_percent: 61.49849624060151\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.5467444732785225\n",
      "    player_1: 4.0564481019973755\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.032877532286303385\n",
      "    player_1: -0.03453995691878455\n",
      "  policy_reward_min:\n",
      "    player_0: -4.067953512072563\n",
      "    player_1: -4.094876199960709\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08101782301224372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0440981807072813\n",
      "    mean_inference_ms: 4.140596024981606\n",
      "    mean_raw_obs_processing_ms: 0.27459095860738053\n",
      "  time_since_restore: 1146.446388244629\n",
      "  time_this_iter_s: 377.42080307006836\n",
      "  time_total_s: 1146.446388244629\n",
      "  timers:\n",
      "    learn_throughput: 10.611\n",
      "    learn_time_ms: 376956.614\n",
      "    sample_throughput: 794.43\n",
      "    sample_time_ms: 5035.058\n",
      "    update_time_ms: 122.785\n",
      "  timestamp: 1627289010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1146.45</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-0.00166242</td><td style=\"text-align: right;\">             0.55763</td><td style=\"text-align: right;\">           -0.548132</td><td style=\"text-align: right;\">           166.743</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 15907\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_01-49-56\n",
      "  done: false\n",
      "  episode_len_mean: 165.09574468085106\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5576299130916595\n",
      "  episode_reward_mean: 0.0004177683211387472\n",
      "  episode_reward_min: -0.5481317266821861\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 94\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.149796962738037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011982064315816388\n",
      "          policy_loss: 0.23989209905266762\n",
      "          total_loss: 0.3087699208408594\n",
      "          vf_explained_var: 0.884534478187561\n",
      "          vf_loss: 0.06348589132539928\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.1026142686605453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009119029738940299\n",
      "          policy_loss: 0.23085936531424522\n",
      "          total_loss: 0.2806495502591133\n",
      "          vf_explained_var: 0.9191637635231018\n",
      "          vf_loss: 0.04568662331439555\n",
      "    num_agent_steps_sampled: 15907\n",
      "    num_agent_steps_trained: 15907\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.65412844036697\n",
      "    ram_util_percent: 59.58532110091743\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.5467444732785225\n",
      "    player_1: 4.0564481019973755\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.022086773781066247\n",
      "    player_1: -0.021669005459927497\n",
      "  policy_reward_min:\n",
      "    player_0: -4.067953512072563\n",
      "    player_1: -4.094876199960709\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08080383704971439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0291559195298963\n",
      "    mean_inference_ms: 4.1647548103867305\n",
      "    mean_raw_obs_processing_ms: 0.2734565102334626\n",
      "  time_since_restore: 1533.0314855575562\n",
      "  time_this_iter_s: 386.58509731292725\n",
      "  time_total_s: 1533.0314855575562\n",
      "  timers:\n",
      "    learn_throughput: 10.578\n",
      "    learn_time_ms: 378141.317\n",
      "    sample_throughput: 803.607\n",
      "    sample_time_ms: 4977.557\n",
      "    update_time_ms: 97.245\n",
      "  timestamp: 1627289396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1533.03</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">0.000417768</td><td style=\"text-align: right;\">             0.55763</td><td style=\"text-align: right;\">           -0.548132</td><td style=\"text-align: right;\">           165.096</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 19884\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_01-56-16\n",
      "  done: false\n",
      "  episode_len_mean: 164.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9071803092956543\n",
      "  episode_reward_mean: -0.005326104164123535\n",
      "  episode_reward_min: -0.9188584983348846\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 119\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.136022225022316\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013263687142170966\n",
      "          policy_loss: 0.24761915858834982\n",
      "          total_loss: 0.34293216466903687\n",
      "          vf_explained_var: 0.8504529595375061\n",
      "          vf_loss: 0.08934433944523335\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.0310121029615402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012689424300333485\n",
      "          policy_loss: 0.2381083369255066\n",
      "          total_loss: 0.3070803377777338\n",
      "          vf_explained_var: 0.8923792839050293\n",
      "          vf_loss: 0.06326176505535841\n",
      "    num_agent_steps_sampled: 19884\n",
      "    num_agent_steps_trained: 19884\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.070411985018728\n",
      "    ram_util_percent: 58.709925093632954\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.210743121802807\n",
      "    player_1: 4.0564481019973755\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.03279962062835693\n",
      "    player_1: 0.027473516464233398\n",
      "  policy_reward_min:\n",
      "    player_0: -4.067953512072563\n",
      "    player_1: -4.17552150785923\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08100576000212534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0084759036737476\n",
      "    mean_inference_ms: 4.180660382692181\n",
      "    mean_raw_obs_processing_ms: 0.27255291720417324\n",
      "  time_since_restore: 1912.3774254322052\n",
      "  time_this_iter_s: 379.34593987464905\n",
      "  time_total_s: 1912.3774254322052\n",
      "  timers:\n",
      "    learn_throughput: 10.599\n",
      "    learn_time_ms: 377405.364\n",
      "    sample_throughput: 808.788\n",
      "    sample_time_ms: 4945.672\n",
      "    update_time_ms: 83.321\n",
      "  timestamp: 1627289776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1912.38</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-0.0053261</td><td style=\"text-align: right;\">             0.90718</td><td style=\"text-align: right;\">           -0.918858</td><td style=\"text-align: right;\">            164.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 23861\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-02-34\n",
      "  done: false\n",
      "  episode_len_mean: 163.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9071803092956543\n",
      "  episode_reward_mean: 0.00013033930445089937\n",
      "  episode_reward_min: -0.9188584983348846\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 145\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.0320945084095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01340043026721105\n",
      "          policy_loss: 0.23650844395160675\n",
      "          total_loss: 0.32034061942249537\n",
      "          vf_explained_var: 0.834111213684082\n",
      "          vf_loss: 0.07780198007822037\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.0065371841192245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014449669455643743\n",
      "          policy_loss: 0.23595320712774992\n",
      "          total_loss: 0.3069725278764963\n",
      "          vf_explained_var: 0.86568284034729\n",
      "          vf_loss: 0.06451696716248989\n",
      "    num_agent_steps_sampled: 23861\n",
      "    num_agent_steps_trained: 23861\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.184052532833022\n",
      "    ram_util_percent: 59.25234521575985\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.210743121802807\n",
      "    player_1: 3.4751611948013306\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.06453592896461487\n",
      "    player_1: 0.06466626826906577\n",
      "  policy_reward_min:\n",
      "    player_0: -3.4726450964808464\n",
      "    player_1: -4.17552150785923\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08060823892096015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9796685704942064\n",
      "    mean_inference_ms: 4.229498826152198\n",
      "    mean_raw_obs_processing_ms: 0.2695486880258102\n",
      "  time_since_restore: 2290.8586745262146\n",
      "  time_this_iter_s: 378.4812490940094\n",
      "  time_total_s: 2290.8586745262146\n",
      "  timers:\n",
      "    learn_throughput: 10.615\n",
      "    learn_time_ms: 376807.978\n",
      "    sample_throughput: 818.905\n",
      "    sample_time_ms: 4884.572\n",
      "    update_time_ms: 74.237\n",
      "  timestamp: 1627290154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         2290.86</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">0.000130339</td><td style=\"text-align: right;\">             0.90718</td><td style=\"text-align: right;\">           -0.918858</td><td style=\"text-align: right;\">            163.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 27841\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 162.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9071803092956543\n",
      "  episode_reward_mean: 0.0018392592668533326\n",
      "  episode_reward_min: -0.9188584983348846\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 169\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.9400500059127808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01711081143002957\n",
      "          policy_loss: 0.2340418230742216\n",
      "          total_loss: 0.3500050902366638\n",
      "          vf_explained_var: 0.8512314558029175\n",
      "          vf_loss: 0.1082634013146162\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.9304270297288895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010608427575789392\n",
      "          policy_loss: 0.2406239891424775\n",
      "          total_loss: 0.3149995543062687\n",
      "          vf_explained_var: 0.8969802260398865\n",
      "          vf_loss: 0.0696017756126821\n",
      "    num_agent_steps_sampled: 27841\n",
      "    num_agent_steps_trained: 27841\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.045386064030133\n",
      "    ram_util_percent: 60.60998116760828\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.210743121802807\n",
      "    player_1: 3.9712533075362444\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.047035672217607495\n",
      "    player_1: 0.04887493148446083\n",
      "  policy_reward_min:\n",
      "    player_0: -4.08496767282486\n",
      "    player_1: -4.17552150785923\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08006014123711683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9633488447503409\n",
      "    mean_inference_ms: 4.2472615194757255\n",
      "    mean_raw_obs_processing_ms: 0.26930456145763165\n",
      "  time_since_restore: 2668.174535036087\n",
      "  time_this_iter_s: 377.31586050987244\n",
      "  time_total_s: 2668.174535036087\n",
      "  timers:\n",
      "    learn_throughput: 10.632\n",
      "    learn_time_ms: 376227.554\n",
      "    sample_throughput: 828.037\n",
      "    sample_time_ms: 4830.704\n",
      "    update_time_ms: 65.077\n",
      "  timestamp: 1627290532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         2668.17</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">0.00183926</td><td style=\"text-align: right;\">             0.90718</td><td style=\"text-align: right;\">           -0.918858</td><td style=\"text-align: right;\">            162.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 31819\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-15-14\n",
      "  done: false\n",
      "  episode_len_mean: 162.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9071803092956543\n",
      "  episode_reward_mean: -0.0026316992938518525\n",
      "  episode_reward_min: -0.9188584983348846\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 192\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.9406385719776154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014365610666573048\n",
      "          policy_loss: 0.23203949723392725\n",
      "          total_loss: 0.3351135775446892\n",
      "          vf_explained_var: 0.8388615846633911\n",
      "          vf_loss: 0.09660955425351858\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.8112177699804306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020249932480510324\n",
      "          policy_loss: 0.21747639030218124\n",
      "          total_loss: 0.29198668897151947\n",
      "          vf_explained_var: 0.8974620699882507\n",
      "          vf_loss: 0.06539782392792404\n",
      "    num_agent_steps_sampled: 31819\n",
      "    num_agent_steps_trained: 31819\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.902973977695167\n",
      "    ram_util_percent: 60.83271375464683\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.210743121802807\n",
      "    player_1: 3.9712533075362444\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.007548983544111252\n",
      "    player_1: -0.010180682837963105\n",
      "  policy_reward_min:\n",
      "    player_0: -4.08496767282486\n",
      "    player_1: -4.17552150785923\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07995811345012022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.954621490810187\n",
      "    mean_inference_ms: 4.242783794156038\n",
      "    mean_raw_obs_processing_ms: 0.26709053995077825\n",
      "  time_since_restore: 3049.8930280208588\n",
      "  time_this_iter_s: 381.71849298477173\n",
      "  time_total_s: 3049.8930280208588\n",
      "  timers:\n",
      "    learn_throughput: 10.629\n",
      "    learn_time_ms: 376345.201\n",
      "    sample_throughput: 835.698\n",
      "    sample_time_ms: 4786.421\n",
      "    update_time_ms: 59.217\n",
      "  timestamp: 1627290914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         3049.89</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.0026317</td><td style=\"text-align: right;\">             0.90718</td><td style=\"text-align: right;\">           -0.918858</td><td style=\"text-align: right;\">            162.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 35795\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-21-34\n",
      "  done: false\n",
      "  episode_len_mean: 162.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9071803092956543\n",
      "  episode_reward_mean: 0.012384618818759918\n",
      "  episode_reward_min: -0.3729225695133209\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 217\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.8688370883464813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017976999166421592\n",
      "          policy_loss: 0.2269759690389037\n",
      "          total_loss: 0.31734666135162115\n",
      "          vf_explained_var: 0.7967200875282288\n",
      "          vf_loss: 0.08228104095906019\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.8069937974214554\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011380150564946234\n",
      "          policy_loss: 0.22062152717262506\n",
      "          total_loss: 0.28941826336085796\n",
      "          vf_explained_var: 0.8632863759994507\n",
      "          vf_loss: 0.06111512705683708\n",
      "    num_agent_steps_sampled: 35795\n",
      "    num_agent_steps_trained: 35795\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.971641791044776\n",
      "    ram_util_percent: 58.17294776119403\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.847841829061508\n",
      "    player_1: 3.9712533075362444\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.0504951012134552\n",
      "    player_1: -0.038110482394695284\n",
      "  policy_reward_min:\n",
      "    player_0: -4.08496767282486\n",
      "    player_1: -2.9184046387672424\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0791504492064254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9520196021513783\n",
      "    mean_inference_ms: 4.224692356647713\n",
      "    mean_raw_obs_processing_ms: 0.2640631005986521\n",
      "  time_since_restore: 3430.658660173416\n",
      "  time_this_iter_s: 380.7656321525574\n",
      "  time_total_s: 3430.658660173416\n",
      "  timers:\n",
      "    learn_throughput: 10.629\n",
      "    learn_time_ms: 376331.514\n",
      "    sample_throughput: 841.85\n",
      "    sample_time_ms: 4751.438\n",
      "    update_time_ms: 54.885\n",
      "  timestamp: 1627291294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         3430.66</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">0.0123846</td><td style=\"text-align: right;\">             0.90718</td><td style=\"text-align: right;\">           -0.372923</td><td style=\"text-align: right;\">            162.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 39772\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-27-49\n",
      "  done: false\n",
      "  episode_len_mean: 166.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39528921246528625\n",
      "  episode_reward_mean: 0.00027446448802948\n",
      "  episode_reward_min: -0.3729225695133209\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 240\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.7596963346004486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019190570223145187\n",
      "          policy_loss: 0.21689125802367926\n",
      "          total_loss: 0.3006228441372514\n",
      "          vf_explained_var: 0.8396494388580322\n",
      "          vf_loss: 0.07509583770297468\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.7587239295244217\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009923115983838215\n",
      "          policy_loss: 0.21344689466059208\n",
      "          total_loss: 0.2860609879717231\n",
      "          vf_explained_var: 0.8797770142555237\n",
      "          vf_loss: 0.06591598689556122\n",
      "    num_agent_steps_sampled: 39772\n",
      "    num_agent_steps_trained: 39772\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.002083333333333\n",
      "    ram_util_percent: 61.09015151515151\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.847841829061508\n",
      "    player_1: 3.9712533075362444\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.020346053540706635\n",
      "    player_1: -0.020071589052677155\n",
      "  policy_reward_min:\n",
      "    player_0: -4.08496767282486\n",
      "    player_1: -2.86019966006279\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07832791418882508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9567774095597514\n",
      "    mean_inference_ms: 4.201814544661598\n",
      "    mean_raw_obs_processing_ms: 0.26205577588828\n",
      "  time_since_restore: 3805.593674182892\n",
      "  time_this_iter_s: 374.9350140094757\n",
      "  time_total_s: 3805.593674182892\n",
      "  timers:\n",
      "    learn_throughput: 10.646\n",
      "    learn_time_ms: 375742.136\n",
      "    sample_throughput: 847.22\n",
      "    sample_time_ms: 4721.324\n",
      "    update_time_ms: 51.278\n",
      "  timestamp: 1627291669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         3805.59</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">0.000274464</td><td style=\"text-align: right;\">            0.395289</td><td style=\"text-align: right;\">           -0.372923</td><td style=\"text-align: right;\">             166.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 43750\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-34-08\n",
      "  done: false\n",
      "  episode_len_mean: 164.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.3079441376030445\n",
      "  episode_reward_mean: -0.001198873668909073\n",
      "  episode_reward_min: -0.29993074014782906\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 265\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000007\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.5985332131385803\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02525810559745878\n",
      "          policy_loss: 0.20140550006181002\n",
      "          total_loss: 0.3264635158702731\n",
      "          vf_explained_var: 0.8032166361808777\n",
      "          vf_loss: 0.11369187943637371\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.8121505230665207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008565254887798801\n",
      "          policy_loss: 0.22905780002474785\n",
      "          total_loss: 0.3068666448816657\n",
      "          vf_explained_var: 0.8641700744628906\n",
      "          vf_loss: 0.07202730653807521\n",
      "    num_agent_steps_sampled: 43750\n",
      "    num_agent_steps_trained: 43750\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.156367041198502\n",
      "    ram_util_percent: 59.58782771535581\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.753751054406166\n",
      "    player_1: 4.130990356206894\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.03209187030792236\n",
      "    player_1: -0.03329074397683143\n",
      "  policy_reward_min:\n",
      "    player_0: -4.096051603555679\n",
      "    player_1: -2.824520990252495\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07727015874655559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9609061854872698\n",
      "    mean_inference_ms: 4.184211668414832\n",
      "    mean_raw_obs_processing_ms: 0.25948274158056633\n",
      "  time_since_restore: 4184.524366617203\n",
      "  time_this_iter_s: 378.9306924343109\n",
      "  time_total_s: 4184.524366617203\n",
      "  timers:\n",
      "    learn_throughput: 10.665\n",
      "    learn_time_ms: 375055.96\n",
      "    sample_throughput: 869.491\n",
      "    sample_time_ms: 4600.391\n",
      "    update_time_ms: 50.782\n",
      "  timestamp: 1627292048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         4184.52</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.00119887</td><td style=\"text-align: right;\">            0.307944</td><td style=\"text-align: right;\">           -0.299931</td><td style=\"text-align: right;\">            164.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 47728\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 162.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.3350786119699478\n",
      "  episode_reward_mean: 0.002907438576221466\n",
      "  episode_reward_min: -0.30772536993026733\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 291\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.658664643764496\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011202807683730498\n",
      "          policy_loss: 0.21770122647285461\n",
      "          total_loss: 0.294051474891603\n",
      "          vf_explained_var: 0.8878992199897766\n",
      "          vf_loss: 0.06878835684619844\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.6566762775182724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012239862029673532\n",
      "          policy_loss: 0.20804011542350054\n",
      "          total_loss: 0.2696006568148732\n",
      "          vf_explained_var: 0.9072259068489075\n",
      "          vf_loss: 0.053298637038096786\n",
      "    num_agent_steps_sampled: 47728\n",
      "    num_agent_steps_trained: 47728\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.951879699248119\n",
      "    ram_util_percent: 60.863345864661646\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.6861321926116943\n",
      "    player_1: 4.130990356206894\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.07122788935899735\n",
      "    player_1: -0.06832045078277588\n",
      "  policy_reward_min:\n",
      "    player_0: -4.096051603555679\n",
      "    player_1: -2.6188623905181885\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0760374536658931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.967871213024246\n",
      "    mean_inference_ms: 4.164663604056924\n",
      "    mean_raw_obs_processing_ms: 0.2581094945923378\n",
      "  time_since_restore: 4562.235119342804\n",
      "  time_this_iter_s: 377.7107527256012\n",
      "  time_total_s: 4562.235119342804\n",
      "  timers:\n",
      "    learn_throughput: 10.677\n",
      "    learn_time_ms: 374638.717\n",
      "    sample_throughput: 872.231\n",
      "    sample_time_ms: 4585.941\n",
      "    update_time_ms: 51.587\n",
      "  timestamp: 1627292426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         4562.24</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">0.00290744</td><td style=\"text-align: right;\">            0.335079</td><td style=\"text-align: right;\">           -0.307725</td><td style=\"text-align: right;\">            162.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 51705\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-46-49\n",
      "  done: false\n",
      "  episode_len_mean: 162.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.3350786119699478\n",
      "  episode_reward_mean: 0.000812993897125125\n",
      "  episode_reward_min: -0.30772536993026733\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 315\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.6028587967157364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012891228368971497\n",
      "          policy_loss: 0.20356919150799513\n",
      "          total_loss: 0.2957883533090353\n",
      "          vf_explained_var: 0.8240993022918701\n",
      "          vf_loss: 0.08351758774369955\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.608384981751442\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009342274657683447\n",
      "          policy_loss: 0.21057435404509306\n",
      "          total_loss: 0.28545793797820807\n",
      "          vf_explained_var: 0.8798399567604065\n",
      "          vf_loss: 0.06857754313386977\n",
      "    num_agent_steps_sampled: 51705\n",
      "    num_agent_steps_trained: 51705\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.930669144981412\n",
      "    ram_util_percent: 61.18513011152416\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.812969848513603\n",
      "    player_1: 4.130990356206894\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.042709472179412844\n",
      "    player_1: -0.041896478282287715\n",
      "  policy_reward_min:\n",
      "    player_0: -4.096051603555679\n",
      "    player_1: -3.7964609265327454\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0750935779136091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.972016238964088\n",
      "    mean_inference_ms: 4.150393118519258\n",
      "    mean_raw_obs_processing_ms: 0.25744002799850096\n",
      "  time_since_restore: 4944.488043785095\n",
      "  time_this_iter_s: 382.25292444229126\n",
      "  time_total_s: 4944.488043785095\n",
      "  timers:\n",
      "    learn_throughput: 10.661\n",
      "    learn_time_ms: 375182.39\n",
      "    sample_throughput: 878.253\n",
      "    sample_time_ms: 4554.496\n",
      "    update_time_ms: 21.224\n",
      "  timestamp: 1627292809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         4944.49</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">0.000812994</td><td style=\"text-align: right;\">            0.335079</td><td style=\"text-align: right;\">           -0.307725</td><td style=\"text-align: right;\">            162.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 55683\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 161.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7909342832863331\n",
      "  episode_reward_mean: -0.003747224807739258\n",
      "  episode_reward_min: -0.763214249163866\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 339\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.5411361902952194\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01195088354870677\n",
      "          policy_loss: 0.2074593175202608\n",
      "          total_loss: 0.2697366503998637\n",
      "          vf_explained_var: 0.8563655614852905\n",
      "          vf_loss: 0.05421048391144723\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.629588395357132\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010689873015508056\n",
      "          policy_loss: 0.212720628362149\n",
      "          total_loss: 0.2521195197477937\n",
      "          vf_explained_var: 0.896668553352356\n",
      "          vf_loss: 0.032183229457587004\n",
      "    num_agent_steps_sampled: 55683\n",
      "    num_agent_steps_trained: 55683\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.99548872180451\n",
      "    ram_util_percent: 60.93327067669173\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.4153220243752\n",
      "    player_1: 4.130990356206894\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.06607842952013016\n",
      "    player_1: -0.06982565432786941\n",
      "  policy_reward_min:\n",
      "    player_0: -4.096051603555679\n",
      "    player_1: -3.7964609265327454\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07463407346005285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9727565867043647\n",
      "    mean_inference_ms: 4.142154473607487\n",
      "    mean_raw_obs_processing_ms: 0.2575819500265342\n",
      "  time_since_restore: 5322.099113702774\n",
      "  time_this_iter_s: 377.61106991767883\n",
      "  time_total_s: 5322.099113702774\n",
      "  timers:\n",
      "    learn_throughput: 10.686\n",
      "    learn_time_ms: 374317.454\n",
      "    sample_throughput: 884.373\n",
      "    sample_time_ms: 4522.977\n",
      "    update_time_ms: 21.582\n",
      "  timestamp: 1627293186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          5322.1</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.00374722</td><td style=\"text-align: right;\">            0.790934</td><td style=\"text-align: right;\">           -0.763214</td><td style=\"text-align: right;\">            161.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 59662\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_02-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 164.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3763539493083954\n",
      "  episode_reward_mean: 0.0004311978816986084\n",
      "  episode_reward_min: -1.3853581994771957\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 363\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.5248454362154007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012446825625374913\n",
      "          policy_loss: 0.20566678047180176\n",
      "          total_loss: 0.29249664954841137\n",
      "          vf_explained_var: 0.8177640438079834\n",
      "          vf_loss: 0.07842825911939144\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.4844953566789627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016287789127090946\n",
      "          policy_loss: 0.19195010792464018\n",
      "          total_loss: 0.2861322471871972\n",
      "          vf_explained_var: 0.8012002110481262\n",
      "          vf_loss: 0.0831878709141165\n",
      "    num_agent_steps_sampled: 59662\n",
      "    num_agent_steps_trained: 59662\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.956983240223463\n",
      "    ram_util_percent: 61.476163873370574\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.4153220243752\n",
      "    player_1: 4.130990356206894\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.0777310685813427\n",
      "    player_1: -0.07729987069964409\n",
      "  policy_reward_min:\n",
      "    player_0: -4.096051603555679\n",
      "    player_1: -3.7964609265327454\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07413391824135099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9722522534448786\n",
      "    mean_inference_ms: 4.135611365528841\n",
      "    mean_raw_obs_processing_ms: 0.2568498958770291\n",
      "  time_since_restore: 5703.501315116882\n",
      "  time_this_iter_s: 381.4022014141083\n",
      "  time_total_s: 5703.501315116882\n",
      "  timers:\n",
      "    learn_throughput: 10.679\n",
      "    learn_time_ms: 374561.92\n",
      "    sample_throughput: 891.827\n",
      "    sample_time_ms: 4485.174\n",
      "    update_time_ms: 20.653\n",
      "  timestamp: 1627293568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">          5703.5</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">0.000431198</td><td style=\"text-align: right;\">             1.37635</td><td style=\"text-align: right;\">            -1.38536</td><td style=\"text-align: right;\">            164.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 63641\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-05-49\n",
      "  done: false\n",
      "  episode_len_mean: 164.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3763539493083954\n",
      "  episode_reward_mean: -0.0008632682263851165\n",
      "  episode_reward_min: -1.3853581994771957\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 388\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.3461432605981827\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012364266091026366\n",
      "          policy_loss: 0.19380967412143946\n",
      "          total_loss: 0.26945517770946026\n",
      "          vf_explained_var: 0.8502097129821777\n",
      "          vf_loss: 0.06729962537065148\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.5321707725524902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015095137641765177\n",
      "          policy_loss: 0.1812613452784717\n",
      "          total_loss: 0.2541828630492091\n",
      "          vf_explained_var: 0.9040282368659973\n",
      "          vf_loss: 0.06273230258375406\n",
      "    num_agent_steps_sampled: 63641\n",
      "    num_agent_steps_trained: 63641\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.125932835820896\n",
      "    ram_util_percent: 59.29944029850746\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.4153220243752\n",
      "    player_1: 3.471166347153485\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.03872793033719063\n",
      "    player_1: 0.03786466211080551\n",
      "  policy_reward_min:\n",
      "    player_0: -3.5003842413425446\n",
      "    player_1: -3.7964609265327454\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07393301507782069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9720890978897639\n",
      "    mean_inference_ms: 4.130596698340029\n",
      "    mean_raw_obs_processing_ms: 0.2564919313275356\n",
      "  time_since_restore: 6084.905399560928\n",
      "  time_this_iter_s: 381.404084444046\n",
      "  time_total_s: 6084.905399560928\n",
      "  timers:\n",
      "    learn_throughput: 10.67\n",
      "    learn_time_ms: 374866.369\n",
      "    sample_throughput: 893.98\n",
      "    sample_time_ms: 4474.373\n",
      "    update_time_ms: 19.099\n",
      "  timestamp: 1627293949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         6084.91</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-0.000863268</td><td style=\"text-align: right;\">             1.37635</td><td style=\"text-align: right;\">            -1.38536</td><td style=\"text-align: right;\">             164.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 67620\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 163.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3763539493083954\n",
      "  episode_reward_mean: -0.010495791537687182\n",
      "  episode_reward_min: -1.3853581994771957\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 412\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.324533224105835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015775291481986642\n",
      "          policy_loss: 0.18777210265398026\n",
      "          total_loss: 0.3136245207861066\n",
      "          vf_explained_var: 0.7587059736251831\n",
      "          vf_loss: 0.11520409723743796\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.2644065767526627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016777195793110877\n",
      "          policy_loss: 0.17831871332600713\n",
      "          total_loss: 0.2649324871599674\n",
      "          vf_explained_var: 0.816059947013855\n",
      "          vf_loss: 0.07528916071169078\n",
      "    num_agent_steps_sampled: 67620\n",
      "    num_agent_steps_trained: 67620\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.13913043478261\n",
      "    ram_util_percent: 61.67051039697542\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.4153220243752\n",
      "    player_1: 3.471166347153485\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.00374695286154747\n",
      "    player_1: -0.014242744399234652\n",
      "  policy_reward_min:\n",
      "    player_0: -3.5003842413425446\n",
      "    player_1: -3.7964609265327454\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07390244446862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9723528269091202\n",
      "    mean_inference_ms: 4.126408710509131\n",
      "    mean_raw_obs_processing_ms: 0.2560266757385543\n",
      "  time_since_restore: 6459.994984865189\n",
      "  time_this_iter_s: 375.08958530426025\n",
      "  time_total_s: 6459.994984865189\n",
      "  timers:\n",
      "    learn_throughput: 10.677\n",
      "    learn_time_ms: 374641.131\n",
      "    sample_throughput: 893.679\n",
      "    sample_time_ms: 4475.882\n",
      "    update_time_ms: 19.896\n",
      "  timestamp: 1627294324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         6459.99</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-0.0104958</td><td style=\"text-align: right;\">             1.37635</td><td style=\"text-align: right;\">            -1.38536</td><td style=\"text-align: right;\">            163.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 71598\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-18-23\n",
      "  done: false\n",
      "  episode_len_mean: 164.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3763539493083954\n",
      "  episode_reward_mean: 0.0033143579959869383\n",
      "  episode_reward_min: -1.3853581994771957\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 436\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.241977274417877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016377863299567252\n",
      "          policy_loss: 0.18101412989199162\n",
      "          total_loss: 0.2572501068934798\n",
      "          vf_explained_var: 0.7797032594680786\n",
      "          vf_loss: 0.06518092448823154\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.2257765382528305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017023649415932596\n",
      "          policy_loss: 0.17242385633289814\n",
      "          total_loss: 0.23512084828689694\n",
      "          vf_explained_var: 0.7853233814239502\n",
      "          vf_loss: 0.051206034142524004\n",
      "    num_agent_steps_sampled: 71598\n",
      "    num_agent_steps_trained: 71598\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.159287054409006\n",
      "    ram_util_percent: 60.853658536585364\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.044554516673088\n",
      "    player_1: 2.814093589782715\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0009026399254798889\n",
      "    player_1: 0.004216997921466827\n",
      "  policy_reward_min:\n",
      "    player_0: -2.494307368993759\n",
      "    player_1: -3.0439889430999756\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07365518428207841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9721890494952835\n",
      "    mean_inference_ms: 4.123871499981654\n",
      "    mean_raw_obs_processing_ms: 0.25505266642307683\n",
      "  time_since_restore: 6838.953987836838\n",
      "  time_this_iter_s: 378.95900297164917\n",
      "  time_total_s: 6838.953987836838\n",
      "  timers:\n",
      "    learn_throughput: 10.685\n",
      "    learn_time_ms: 374368.318\n",
      "    sample_throughput: 894.34\n",
      "    sample_time_ms: 4472.574\n",
      "    update_time_ms: 20.137\n",
      "  timestamp: 1627294703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         6838.95</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">0.00331436</td><td style=\"text-align: right;\">             1.37635</td><td style=\"text-align: right;\">            -1.38536</td><td style=\"text-align: right;\">            164.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 75574\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-24-40\n",
      "  done: false\n",
      "  episode_len_mean: 163.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2839394062757492\n",
      "  episode_reward_mean: 0.0011795565485954284\n",
      "  episode_reward_min: -1.276942864060402\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 462\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.121547281742096\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015625798900146037\n",
      "          policy_loss: 0.16834120685234666\n",
      "          total_loss: 0.2631753711029887\n",
      "          vf_explained_var: 0.807925820350647\n",
      "          vf_loss: 0.08428675029426813\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.197296053171158\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01721815305063501\n",
      "          policy_loss: 0.1773283127695322\n",
      "          total_loss: 0.26834891084581614\n",
      "          vf_explained_var: 0.8239874839782715\n",
      "          vf_loss: 0.07939833798445761\n",
      "    num_agent_steps_sampled: 75574\n",
      "    num_agent_steps_trained: 75574\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.751977401129944\n",
      "    ram_util_percent: 61.24651600753296\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.044554516673088\n",
      "    player_1: 3.9090558141469955\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0462741108238697\n",
      "    player_1: 0.04745366737246513\n",
      "  policy_reward_min:\n",
      "    player_0: -3.856470175087452\n",
      "    player_1: -3.0439889430999756\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07337658970394867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9721231897535836\n",
      "    mean_inference_ms: 4.121759633441186\n",
      "    mean_raw_obs_processing_ms: 0.2540602266798464\n",
      "  time_since_restore: 7215.323560476303\n",
      "  time_this_iter_s: 376.36957263946533\n",
      "  time_total_s: 7215.323560476303\n",
      "  timers:\n",
      "    learn_throughput: 10.697\n",
      "    learn_time_ms: 373932.764\n",
      "    sample_throughput: 895.15\n",
      "    sample_time_ms: 4468.527\n",
      "    update_time_ms: 19.935\n",
      "  timestamp: 1627295080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         7215.32</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">0.00117956</td><td style=\"text-align: right;\">             1.28394</td><td style=\"text-align: right;\">            -1.27694</td><td style=\"text-align: right;\">            163.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 79552\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-30-59\n",
      "  done: false\n",
      "  episode_len_mean: 163.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2839394062757492\n",
      "  episode_reward_mean: -0.00028880387544631957\n",
      "  episode_reward_min: -1.276942864060402\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 487\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.0392955243587494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01735805784119293\n",
      "          policy_loss: 0.16300051170401275\n",
      "          total_loss: 0.27337853237986565\n",
      "          vf_explained_var: 0.8249858617782593\n",
      "          vf_loss: 0.098661326803267\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.1476297974586487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016363391652703285\n",
      "          policy_loss: 0.17507005389779806\n",
      "          total_loss: 0.2900104094296694\n",
      "          vf_explained_var: 0.830683171749115\n",
      "          vf_loss: 0.1038950679358095\n",
      "    num_agent_steps_sampled: 79552\n",
      "    num_agent_steps_trained: 79552\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.459849906191371\n",
      "    ram_util_percent: 61.79287054409004\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.9710501432418823\n",
      "    player_1: 3.9090558141469955\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.05199791043996811\n",
      "    player_1: 0.05170910656452179\n",
      "  policy_reward_min:\n",
      "    player_0: -3.856470175087452\n",
      "    player_1: -2.957354485988617\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07313644879594254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9710984676518446\n",
      "    mean_inference_ms: 4.120628267759119\n",
      "    mean_raw_obs_processing_ms: 0.253291304914399\n",
      "  time_since_restore: 7594.127007007599\n",
      "  time_this_iter_s: 378.8034465312958\n",
      "  time_total_s: 7594.127007007599\n",
      "  timers:\n",
      "    learn_throughput: 10.686\n",
      "    learn_time_ms: 374312.4\n",
      "    sample_throughput: 894.287\n",
      "    sample_time_ms: 4472.836\n",
      "    update_time_ms: 19.861\n",
      "  timestamp: 1627295459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         7594.13</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-0.000288804</td><td style=\"text-align: right;\">             1.28394</td><td style=\"text-align: right;\">            -1.27694</td><td style=\"text-align: right;\">            163.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 83531\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-37-20\n",
      "  done: false\n",
      "  episode_len_mean: 165.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9551995247602463\n",
      "  episode_reward_mean: 0.00849531516432762\n",
      "  episode_reward_min: -0.8594801276922226\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 508\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.9808035716414452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019486898032482713\n",
      "          policy_loss: 0.14318452589213848\n",
      "          total_loss: 0.23723811376839876\n",
      "          vf_explained_var: 0.8523565530776978\n",
      "          vf_loss: 0.08089993079192936\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.009227231144905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017116679635364562\n",
      "          policy_loss: 0.15290890354663134\n",
      "          total_loss: 0.24370224587619305\n",
      "          vf_explained_var: 0.8770825266838074\n",
      "          vf_loss: 0.07923958380706608\n",
      "    num_agent_steps_sampled: 83531\n",
      "    num_agent_steps_trained: 83531\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.767039106145251\n",
      "    ram_util_percent: 58.90148975791434\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.146656844764948\n",
      "    player_1: 3.9090558141469955\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.06625767216086388\n",
      "    player_1: 0.07475298732519149\n",
      "  policy_reward_min:\n",
      "    player_0: -3.856470175087452\n",
      "    player_1: -3.1103183329105377\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07335290234111617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9705305784097341\n",
      "    mean_inference_ms: 4.117951875640057\n",
      "    mean_raw_obs_processing_ms: 0.25327709437200646\n",
      "  time_since_restore: 7975.38964176178\n",
      "  time_this_iter_s: 381.2626347541809\n",
      "  time_total_s: 7975.38964176178\n",
      "  timers:\n",
      "    learn_throughput: 10.679\n",
      "    learn_time_ms: 374549.787\n",
      "    sample_throughput: 895.28\n",
      "    sample_time_ms: 4467.877\n",
      "    update_time_ms: 20.76\n",
      "  timestamp: 1627295840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         7975.39</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">0.00849532</td><td style=\"text-align: right;\">              0.9552</td><td style=\"text-align: right;\">            -0.85948</td><td style=\"text-align: right;\">             165.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 87507\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-43-35\n",
      "  done: false\n",
      "  episode_len_mean: 164.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7131193578243256\n",
      "  episode_reward_mean: -0.0003279343247413635\n",
      "  episode_reward_min: -0.6151993572711945\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 534\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.9495989009737968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01783649151911959\n",
      "          policy_loss: 0.14689700934104621\n",
      "          total_loss: 0.23295787442475557\n",
      "          vf_explained_var: 0.8215059638023376\n",
      "          vf_loss: 0.0740212332457304\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.8999655097723007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01794924889691174\n",
      "          policy_loss: 0.15755969425663352\n",
      "          total_loss: 0.27682703640311956\n",
      "          vf_explained_var: 0.8331227898597717\n",
      "          vf_loss: 0.10715160192921758\n",
      "    num_agent_steps_sampled: 87507\n",
      "    num_agent_steps_trained: 87507\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.606072106261859\n",
      "    ram_util_percent: 62.23681214421253\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.146656844764948\n",
      "    player_1: 3.9090558141469955\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.05748583197593689\n",
      "    player_1: 0.05715789765119553\n",
      "  policy_reward_min:\n",
      "    player_0: -3.856470175087452\n",
      "    player_1: -3.1103183329105377\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07343980493560548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9710028151206458\n",
      "    mean_inference_ms: 4.114102895428327\n",
      "    mean_raw_obs_processing_ms: 0.2525996084234167\n",
      "  time_since_restore: 8350.21756529808\n",
      "  time_this_iter_s: 374.82792353630066\n",
      "  time_total_s: 8350.21756529808\n",
      "  timers:\n",
      "    learn_throughput: 10.688\n",
      "    learn_time_ms: 374259.818\n",
      "    sample_throughput: 894.951\n",
      "    sample_time_ms: 4469.517\n",
      "    update_time_ms: 19.639\n",
      "  timestamp: 1627296215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         8350.22</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-0.000327934</td><td style=\"text-align: right;\">            0.713119</td><td style=\"text-align: right;\">           -0.615199</td><td style=\"text-align: right;\">            164.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 91485\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-49-56\n",
      "  done: false\n",
      "  episode_len_mean: 164.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7131193578243256\n",
      "  episode_reward_mean: -0.005532904900610447\n",
      "  episode_reward_min: -0.5424053966999054\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 557\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.847617968916893\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018187083129305393\n",
      "          policy_loss: 0.12917444109916687\n",
      "          total_loss: 0.24733708798885345\n",
      "          vf_explained_var: 0.8182804584503174\n",
      "          vf_loss: 0.1058863669168204\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.8109681233763695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017917816701810807\n",
      "          policy_loss: 0.1287661986425519\n",
      "          total_loss: 0.24307926557958126\n",
      "          vf_explained_var: 0.8444034457206726\n",
      "          vf_loss: 0.10221853759139776\n",
      "    num_agent_steps_sampled: 91485\n",
      "    num_agent_steps_trained: 91485\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.715456238361268\n",
      "    ram_util_percent: 60.31824953445065\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.146656844764948\n",
      "    player_1: 3.508506566286087\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0708076498284936\n",
      "    player_1: 0.06527474492788315\n",
      "  policy_reward_min:\n",
      "    player_0: -3.517177663743496\n",
      "    player_1: -3.1103183329105377\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07347799698022107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9701918216103452\n",
      "    mean_inference_ms: 4.112451155695451\n",
      "    mean_raw_obs_processing_ms: 0.25255393083418715\n",
      "  time_since_restore: 8731.200751543045\n",
      "  time_this_iter_s: 380.9831862449646\n",
      "  time_total_s: 8731.200751543045\n",
      "  timers:\n",
      "    learn_throughput: 10.692\n",
      "    learn_time_ms: 374126.564\n",
      "    sample_throughput: 893.654\n",
      "    sample_time_ms: 4476.004\n",
      "    update_time_ms: 19.753\n",
      "  timestamp: 1627296596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          8731.2</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-0.0055329</td><td style=\"text-align: right;\">            0.713119</td><td style=\"text-align: right;\">           -0.542405</td><td style=\"text-align: right;\">             164.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 95461\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_03-56-13\n",
      "  done: false\n",
      "  episode_len_mean: 166.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7131193578243256\n",
      "  episode_reward_mean: 0.0005707845091819763\n",
      "  episode_reward_min: -0.5424053966999054\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 582\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.8018948584794998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013654735230375081\n",
      "          policy_loss: 0.12939692079089582\n",
      "          total_loss: 0.18475168524309993\n",
      "          vf_explained_var: 0.7456867694854736\n",
      "          vf_loss: 0.04613781673833728\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.8239087834954262\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01789307495346293\n",
      "          policy_loss: 0.12495819269679487\n",
      "          total_loss: 0.17876268783584237\n",
      "          vf_explained_var: 0.8250718116760254\n",
      "          vf_loss: 0.04172666813246906\n",
      "    num_agent_steps_sampled: 95461\n",
      "    num_agent_steps_trained: 95461\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.553396226415092\n",
      "    ram_util_percent: 61.88301886792453\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.146656844764948\n",
      "    player_1: 3.508506566286087\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.016283478140830993\n",
      "    player_1: -0.015712693631649018\n",
      "  policy_reward_min:\n",
      "    player_0: -3.4933809377253056\n",
      "    player_1: -3.1103183329105377\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07377008286612631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9711850035057051\n",
      "    mean_inference_ms: 4.109895585235011\n",
      "    mean_raw_obs_processing_ms: 0.2519884538058157\n",
      "  time_since_restore: 9108.283366680145\n",
      "  time_this_iter_s: 377.0826151371002\n",
      "  time_total_s: 9108.283366680145\n",
      "  timers:\n",
      "    learn_throughput: 10.693\n",
      "    learn_time_ms: 374072.398\n",
      "    sample_throughput: 893.301\n",
      "    sample_time_ms: 4477.772\n",
      "    update_time_ms: 20.457\n",
      "  timestamp: 1627296973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         9108.28</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">0.000570785</td><td style=\"text-align: right;\">            0.713119</td><td style=\"text-align: right;\">           -0.542405</td><td style=\"text-align: right;\">            166.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 99438\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-02-33\n",
      "  done: false\n",
      "  episode_len_mean: 165.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7131193578243256\n",
      "  episode_reward_mean: 0.000856366939842701\n",
      "  episode_reward_min: -0.9960585460066795\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 606\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.7476862370967865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018496771401260048\n",
      "          policy_loss: 0.12501053628511727\n",
      "          total_loss: 0.20573822176083922\n",
      "          vf_explained_var: 0.8250178694725037\n",
      "          vf_loss: 0.06824236619286239\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.665458768606186\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019461653020698577\n",
      "          policy_loss: 0.12920499872416258\n",
      "          total_loss: 0.1883976710960269\n",
      "          vf_explained_var: 0.8464706540107727\n",
      "          vf_loss: 0.04605605988763273\n",
      "    num_agent_steps_sampled: 99438\n",
      "    num_agent_steps_trained: 99438\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.618878504672898\n",
      "    ram_util_percent: 61.789532710280376\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.146656844764948\n",
      "    player_1: 3.470582067966461\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.026913557536900043\n",
      "    player_1: -0.026057190597057342\n",
      "  policy_reward_min:\n",
      "    player_0: -3.4586896933615208\n",
      "    player_1: -3.1103183329105377\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07382489574331942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9713778329864721\n",
      "    mean_inference_ms: 4.108827219012463\n",
      "    mean_raw_obs_processing_ms: 0.25104919540069803\n",
      "  time_since_restore: 9488.159220695496\n",
      "  time_this_iter_s: 379.87585401535034\n",
      "  time_total_s: 9488.159220695496\n",
      "  timers:\n",
      "    learn_throughput: 10.698\n",
      "    learn_time_ms: 373909.736\n",
      "    sample_throughput: 891.895\n",
      "    sample_time_ms: 4484.831\n",
      "    update_time_ms: 21.801\n",
      "  timestamp: 1627297353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         9488.16</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">0.000856367</td><td style=\"text-align: right;\">            0.713119</td><td style=\"text-align: right;\">           -0.996059</td><td style=\"text-align: right;\">            165.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 103415\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-08-50\n",
      "  done: false\n",
      "  episode_len_mean: 166.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6873968243598938\n",
      "  episode_reward_mean: -0.00043884068727493284\n",
      "  episode_reward_min: -0.9960585460066795\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 630\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.6283072605729103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01699730783002451\n",
      "          policy_loss: 0.10434567066840827\n",
      "          total_loss: 0.17524190992116928\n",
      "          vf_explained_var: 0.8015459775924683\n",
      "          vf_loss: 0.05942305573262274\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.7260495200753212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014979810977820307\n",
      "          policy_loss: 0.11066852923249826\n",
      "          total_loss: 0.17509259562939405\n",
      "          vf_explained_var: 0.846433162689209\n",
      "          vf_loss: 0.05431268992833793\n",
      "    num_agent_steps_sampled: 103415\n",
      "    num_agent_steps_trained: 103415\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.574011299435027\n",
      "    ram_util_percent: 62.02787193973635\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.0449434742331505\n",
      "    player_1: 3.470582067966461\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.0011141926050186157\n",
      "    player_1: -0.0015530332922935486\n",
      "  policy_reward_min:\n",
      "    player_0: -3.4586896933615208\n",
      "    player_1: -2.842821776866913\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07404053742635669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9706360256970211\n",
      "    mean_inference_ms: 4.109452425890741\n",
      "    mean_raw_obs_processing_ms: 0.24990950069386705\n",
      "  time_since_restore: 9864.89474606514\n",
      "  time_this_iter_s: 376.73552536964417\n",
      "  time_total_s: 9864.89474606514\n",
      "  timers:\n",
      "    learn_throughput: 10.711\n",
      "    learn_time_ms: 373439.504\n",
      "    sample_throughput: 891.174\n",
      "    sample_time_ms: 4488.46\n",
      "    update_time_ms: 21.49\n",
      "  timestamp: 1627297730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         9864.89</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">-0.000438841</td><td style=\"text-align: right;\">            0.687397</td><td style=\"text-align: right;\">           -0.996059</td><td style=\"text-align: right;\">            166.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 107392\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 166.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6873968243598938\n",
      "  episode_reward_mean: -0.001523762010037899\n",
      "  episode_reward_min: -0.9960585460066795\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 654\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.5910507142543793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017119448632001877\n",
      "          policy_loss: 0.1099407181609422\n",
      "          total_loss: 0.18457771325483918\n",
      "          vf_explained_var: 0.755502462387085\n",
      "          vf_loss: 0.06308136763982475\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.5814979523420334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018310524814296514\n",
      "          policy_loss: 0.1061051485594362\n",
      "          total_loss: 0.22088804235681891\n",
      "          vf_explained_var: 0.6539024114608765\n",
      "          vf_loss: 0.10242328373715281\n",
      "    num_agent_steps_sampled: 107392\n",
      "    num_agent_steps_trained: 107392\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.557865168539324\n",
      "    ram_util_percent: 62.14644194756555\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.0449434742331505\n",
      "    player_1: 3.314899429678917\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.004175047017633915\n",
      "    player_1: -0.005698809027671814\n",
      "  policy_reward_min:\n",
      "    player_0: -3.264581263065338\n",
      "    player_1: -2.842821776866913\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07409313704274242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9715972077613302\n",
      "    mean_inference_ms: 4.108735956768916\n",
      "    mean_raw_obs_processing_ms: 0.24872526781444151\n",
      "  time_since_restore: 10244.991531610489\n",
      "  time_this_iter_s: 380.0967855453491\n",
      "  time_total_s: 10244.991531610489\n",
      "  timers:\n",
      "    learn_throughput: 10.697\n",
      "    learn_time_ms: 373940.271\n",
      "    sample_throughput: 891.538\n",
      "    sample_time_ms: 4486.627\n",
      "    update_time_ms: 21.527\n",
      "  timestamp: 1627298110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">           10245</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-0.00152376</td><td style=\"text-align: right;\">            0.687397</td><td style=\"text-align: right;\">           -0.996059</td><td style=\"text-align: right;\">            166.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 111369\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-21-30\n",
      "  done: false\n",
      "  episode_len_mean: 165.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6873968243598938\n",
      "  episode_reward_mean: -0.00250115692615509\n",
      "  episode_reward_min: -0.9960585460066795\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 679\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.4987719058990479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015977214614395052\n",
      "          policy_loss: 0.10172899338067509\n",
      "          total_loss: 0.20405222102999687\n",
      "          vf_explained_var: 0.7969744801521301\n",
      "          vf_loss: 0.09153860807418823\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.5545580759644508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018475650169420987\n",
      "          policy_loss: 0.10574161540716887\n",
      "          total_loss: 0.19684858806431293\n",
      "          vf_explained_var: 0.8379033803939819\n",
      "          vf_loss: 0.07863591215573251\n",
      "    num_agent_steps_sampled: 111369\n",
      "    num_agent_steps_trained: 111369\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.665858208955223\n",
      "    ram_util_percent: 59.54458955223881\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.1192613393068314\n",
      "    player_1: 3.314899429678917\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.003096330463886261\n",
      "    player_1: -0.005597487390041352\n",
      "  policy_reward_min:\n",
      "    player_0: -3.264581263065338\n",
      "    player_1: -3.2471607625484467\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07367931778392561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9712559173531509\n",
      "    mean_inference_ms: 4.108800619709279\n",
      "    mean_raw_obs_processing_ms: 0.24751341464590676\n",
      "  time_since_restore: 10625.372756958008\n",
      "  time_this_iter_s: 380.3812253475189\n",
      "  time_total_s: 10625.372756958008\n",
      "  timers:\n",
      "    learn_throughput: 10.693\n",
      "    learn_time_ms: 374081.113\n",
      "    sample_throughput: 891.172\n",
      "    sample_time_ms: 4488.471\n",
      "    update_time_ms: 20.47\n",
      "  timestamp: 1627298490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         10625.4</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-0.00250116</td><td style=\"text-align: right;\">            0.687397</td><td style=\"text-align: right;\">           -0.996059</td><td style=\"text-align: right;\">            165.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 115346\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-27-46\n",
      "  done: false\n",
      "  episode_len_mean: 166.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6222594901919365\n",
      "  episode_reward_mean: 1.7218291759490967e-05\n",
      "  episode_reward_min: -0.9960585460066795\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 702\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.39481832832098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017836511367931962\n",
      "          policy_loss: 0.09435273124836385\n",
      "          total_loss: 0.21550889313220978\n",
      "          vf_explained_var: 0.7634519338607788\n",
      "          vf_loss: 0.10911651747301221\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.3980334401130676\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02170910034328699\n",
      "          policy_loss: 0.09438667865470052\n",
      "          total_loss: 0.20729616726748645\n",
      "          vf_explained_var: 0.8136053681373596\n",
      "          vf_loss: 0.0982558517716825\n",
      "    num_agent_steps_sampled: 115346\n",
      "    num_agent_steps_trained: 115346\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.762121212121212\n",
      "    ram_util_percent: 62.492613636363636\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.3260882645845413\n",
      "    player_1: 3.097698912024498\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.021775168478488923\n",
      "    player_1: 0.021792386770248414\n",
      "  policy_reward_min:\n",
      "    player_0: -3.1645825505256653\n",
      "    player_1: -3.3264121413230896\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07348236617424457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9700773799528188\n",
      "    mean_inference_ms: 4.110672459927844\n",
      "    mean_raw_obs_processing_ms: 0.24656836046889022\n",
      "  time_since_restore: 11000.508749485016\n",
      "  time_this_iter_s: 375.13599252700806\n",
      "  time_total_s: 11000.508749485016\n",
      "  timers:\n",
      "    learn_throughput: 10.697\n",
      "    learn_time_ms: 373948.156\n",
      "    sample_throughput: 889.215\n",
      "    sample_time_ms: 4498.351\n",
      "    update_time_ms: 20.191\n",
      "  timestamp: 1627298866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         11000.5</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">1.72183e-05</td><td style=\"text-align: right;\">            0.622259</td><td style=\"text-align: right;\">           -0.996059</td><td style=\"text-align: right;\">             166.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 119323\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-34-05\n",
      "  done: false\n",
      "  episode_len_mean: 166.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6099645793437958\n",
      "  episode_reward_mean: -0.004338023415766656\n",
      "  episode_reward_min: -0.675106942653656\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 726\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.3585397377610207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021098376309964806\n",
      "          policy_loss: 0.09323408972704783\n",
      "          total_loss: 0.2310422477312386\n",
      "          vf_explained_var: 0.6507806181907654\n",
      "          vf_loss: 0.12356675742194057\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.4586302116513252\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011170552344992757\n",
      "          policy_loss: 0.09978705761022866\n",
      "          total_loss: 0.2817637035623193\n",
      "          vf_explained_var: 0.49728962779045105\n",
      "          vf_loss: 0.1706664664670825\n",
      "    num_agent_steps_sampled: 119323\n",
      "    num_agent_steps_trained: 119323\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.969475655430713\n",
      "    ram_util_percent: 61.26554307116105\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.3260882645845413\n",
      "    player_1: 3.0523712560534477\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.06891444092150778\n",
      "    player_1: 0.06457641750574111\n",
      "  policy_reward_min:\n",
      "    player_0: -3.051628617104143\n",
      "    player_1: -3.3264121413230896\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0726550764202685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9698052114319092\n",
      "    mean_inference_ms: 4.110593345612111\n",
      "    mean_raw_obs_processing_ms: 0.24625359904282185\n",
      "  time_since_restore: 11379.927250623703\n",
      "  time_this_iter_s: 379.41850113868713\n",
      "  time_total_s: 11379.927250623703\n",
      "  timers:\n",
      "    learn_throughput: 10.695\n",
      "    learn_time_ms: 374017.568\n",
      "    sample_throughput: 890.38\n",
      "    sample_time_ms: 4492.462\n",
      "    update_time_ms: 19.95\n",
      "  timestamp: 1627299245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         11379.9</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-0.00433802</td><td style=\"text-align: right;\">            0.609965</td><td style=\"text-align: right;\">           -0.675107</td><td style=\"text-align: right;\">            166.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 123299\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 165.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5103340744972229\n",
      "  episode_reward_mean: 0.00601176917552948\n",
      "  episode_reward_min: -0.4690195620059967\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 751\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.3293742164969444\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012290777056477964\n",
      "          policy_loss: 0.0759044298902154\n",
      "          total_loss: 0.17309262417256832\n",
      "          vf_explained_var: 0.7882711291313171\n",
      "          vf_loss: 0.0847437831107527\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.4027754664421082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012071886274497956\n",
      "          policy_loss: 0.08248321461724117\n",
      "          total_loss: 0.16990801552310586\n",
      "          vf_explained_var: 0.8151949644088745\n",
      "          vf_loss: 0.07520201778970659\n",
      "    num_agent_steps_sampled: 123299\n",
      "    num_agent_steps_trained: 123299\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.919962335216573\n",
      "    ram_util_percent: 62.12184557438794\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.3260882645845413\n",
      "    player_1: 3.0523712560534477\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.03989673942327499\n",
      "    player_1: 0.04590850859880447\n",
      "  policy_reward_min:\n",
      "    player_0: -3.051628617104143\n",
      "    player_1: -3.3264121413230896\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07224620117280002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9694666810998358\n",
      "    mean_inference_ms: 4.111494069625503\n",
      "    mean_raw_obs_processing_ms: 0.24562336803401622\n",
      "  time_since_restore: 11757.367072343826\n",
      "  time_this_iter_s: 377.4398217201233\n",
      "  time_total_s: 11757.367072343826\n",
      "  timers:\n",
      "    learn_throughput: 10.706\n",
      "    learn_time_ms: 373626.078\n",
      "    sample_throughput: 888.796\n",
      "    sample_time_ms: 4500.468\n",
      "    update_time_ms: 18.789\n",
      "  timestamp: 1627299623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         11757.4</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">0.00601177</td><td style=\"text-align: right;\">            0.510334</td><td style=\"text-align: right;\">            -0.46902</td><td style=\"text-align: right;\">             165.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 127278\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 166.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5126515030860901\n",
      "  episode_reward_mean: 0.0018653854727745055\n",
      "  episode_reward_min: -0.4690195620059967\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 774\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.2959352284669876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011706114339176565\n",
      "          policy_loss: 0.09305524069350213\n",
      "          total_loss: 0.22983677173033357\n",
      "          vf_explained_var: 0.7763849496841431\n",
      "          vf_loss: 0.12492909329012036\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.345268838107586\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011022165796021\n",
      "          policy_loss: 0.08120182249695063\n",
      "          total_loss: 0.22122622467577457\n",
      "          vf_explained_var: 0.763688325881958\n",
      "          vf_loss: 0.12886445876210928\n",
      "    num_agent_steps_sampled: 127278\n",
      "    num_agent_steps_trained: 127278\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.551115241635689\n",
      "    ram_util_percent: 62.80687732342008\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.1178108006715775\n",
      "    player_1: 3.0523712560534477\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.01834867715835571\n",
      "    player_1: 0.02021406263113022\n",
      "  policy_reward_min:\n",
      "    player_0: -3.051628617104143\n",
      "    player_1: -4.0934450179338455\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07213480036505991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9695684372936384\n",
      "    mean_inference_ms: 4.111091211101308\n",
      "    mean_raw_obs_processing_ms: 0.24594611046760234\n",
      "  time_since_restore: 12139.187016248703\n",
      "  time_this_iter_s: 381.8199439048767\n",
      "  time_total_s: 12139.187016248703\n",
      "  timers:\n",
      "    learn_throughput: 10.686\n",
      "    learn_time_ms: 374322.16\n",
      "    sample_throughput: 888.6\n",
      "    sample_time_ms: 4501.463\n",
      "    update_time_ms: 19.65\n",
      "  timestamp: 1627300004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         12139.2</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">0.00186539</td><td style=\"text-align: right;\">            0.512652</td><td style=\"text-align: right;\">            -0.46902</td><td style=\"text-align: right;\">            166.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 131253\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 166.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5126515030860901\n",
      "  episode_reward_mean: -0.0025054562836885454\n",
      "  episode_reward_min: -0.4690195620059967\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 799\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.2343087419867516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010133193951332942\n",
      "          policy_loss: 0.06223591428715736\n",
      "          total_loss: 0.14692929957527667\n",
      "          vf_explained_var: 0.696048378944397\n",
      "          vf_loss: 0.074433522997424\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.3494541868567467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010461592522915453\n",
      "          policy_loss: 0.08023878914536908\n",
      "          total_loss: 0.1404792082030326\n",
      "          vf_explained_var: 0.8406075835227966\n",
      "          vf_loss: 0.04964805860072374\n",
      "    num_agent_steps_sampled: 131253\n",
      "    num_agent_steps_trained: 131253\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.594972067039107\n",
      "    ram_util_percent: 59.23836126629423\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.1178108006715775\n",
      "    player_1: 3.0523712560534477\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0262354002147913\n",
      "    player_1: 0.023729943931102754\n",
      "  policy_reward_min:\n",
      "    player_0: -3.051628617104143\n",
      "    player_1: -4.0934450179338455\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07223762812147727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.970668670581432\n",
      "    mean_inference_ms: 4.109036022847005\n",
      "    mean_raw_obs_processing_ms: 0.24581025002284718\n",
      "  time_since_restore: 12520.305060863495\n",
      "  time_this_iter_s: 381.11804461479187\n",
      "  time_total_s: 12520.305060863495\n",
      "  timers:\n",
      "    learn_throughput: 10.685\n",
      "    learn_time_ms: 374340.296\n",
      "    sample_throughput: 889.957\n",
      "    sample_time_ms: 4494.6\n",
      "    update_time_ms: 19.574\n",
      "  timestamp: 1627300386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         12520.3</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-0.00250546</td><td style=\"text-align: right;\">            0.512652</td><td style=\"text-align: right;\">            -0.46902</td><td style=\"text-align: right;\">            166.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 135231\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_04-59-20\n",
      "  done: false\n",
      "  episode_len_mean: 166.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5126515030860901\n",
      "  episode_reward_mean: 0.0035018805181607606\n",
      "  episode_reward_min: -0.3300616145133972\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 824\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.1663584932684898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012267777521628886\n",
      "          policy_loss: 0.057156573311658576\n",
      "          total_loss: 0.14020774164237082\n",
      "          vf_explained_var: 0.7879290580749512\n",
      "          vf_loss: 0.07063004141673446\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.2135619893670082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01074126287130639\n",
      "          policy_loss: 0.06895367638207972\n",
      "          total_loss: 0.1575073127169162\n",
      "          vf_explained_var: 0.8356930017471313\n",
      "          vf_loss: 0.07767811068333685\n",
      "    num_agent_steps_sampled: 135231\n",
      "    num_agent_steps_trained: 135231\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.544592030360532\n",
      "    ram_util_percent: 62.522390891840615\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.1178108006715775\n",
      "    player_1: 2.7208639681339264\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.027844649902544916\n",
      "    player_1: -0.024342769384384157\n",
      "  policy_reward_min:\n",
      "    player_0: -2.7661631256341934\n",
      "    player_1: -4.0934450179338455\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0727912080100623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.971495058574483\n",
      "    mean_inference_ms: 4.107651398248262\n",
      "    mean_raw_obs_processing_ms: 0.24565778532302535\n",
      "  time_since_restore: 12894.89694237709\n",
      "  time_this_iter_s: 374.5918815135956\n",
      "  time_total_s: 12894.89694237709\n",
      "  timers:\n",
      "    learn_throughput: 10.693\n",
      "    learn_time_ms: 374093.143\n",
      "    sample_throughput: 889.791\n",
      "    sample_time_ms: 4495.438\n",
      "    update_time_ms: 18.322\n",
      "  timestamp: 1627300760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         12894.9</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">0.00350188</td><td style=\"text-align: right;\">            0.512652</td><td style=\"text-align: right;\">           -0.330062</td><td style=\"text-align: right;\">            166.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 139210\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-05-41\n",
      "  done: false\n",
      "  episode_len_mean: 165.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5126515030860901\n",
      "  episode_reward_mean: -0.0006059989333152771\n",
      "  episode_reward_min: -0.3300616145133972\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 847\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.1884133219718933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014144547400064766\n",
      "          policy_loss: 0.05569313751766458\n",
      "          total_loss: 0.13644980953540653\n",
      "          vf_explained_var: 0.7024601101875305\n",
      "          vf_loss: 0.0664353147149086\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.231466457247734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013422669260762632\n",
      "          policy_loss: 0.06457721485639922\n",
      "          total_loss: 0.12902679620310664\n",
      "          vf_explained_var: 0.7986925840377808\n",
      "          vf_loss: 0.050859129056334496\n",
      "    num_agent_steps_sampled: 139210\n",
      "    num_agent_steps_trained: 139210\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.835327102803738\n",
      "    ram_util_percent: 60.86878504672898\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 4.1178108006715775\n",
      "    player_1: 2.7208639681339264\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.03550787091255188\n",
      "    player_1: -0.036113869845867154\n",
      "  policy_reward_min:\n",
      "    player_0: -2.7661631256341934\n",
      "    player_1: -4.0934450179338455\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07316765332739561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9714584685093368\n",
      "    mean_inference_ms: 4.105486943397463\n",
      "    mean_raw_obs_processing_ms: 0.24593191628228603\n",
      "  time_since_restore: 13275.175812959671\n",
      "  time_this_iter_s: 380.27887058258057\n",
      "  time_total_s: 13275.175812959671\n",
      "  timers:\n",
      "    learn_throughput: 10.691\n",
      "    learn_time_ms: 374139.637\n",
      "    sample_throughput: 890.389\n",
      "    sample_time_ms: 4492.417\n",
      "    update_time_ms: 16.961\n",
      "  timestamp: 1627301141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         13275.2</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-0.000605999</td><td style=\"text-align: right;\">            0.512652</td><td style=\"text-align: right;\">           -0.330062</td><td style=\"text-align: right;\">            165.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 143186\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-11-57\n",
      "  done: false\n",
      "  episode_len_mean: 166.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4876578599214554\n",
      "  episode_reward_mean: -0.006265918165445328\n",
      "  episode_reward_min: -0.4911380261182785\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 871\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.1754895076155663\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016007139871362597\n",
      "          policy_loss: 0.05244946334278211\n",
      "          total_loss: 0.1463182977749966\n",
      "          vf_explained_var: 0.6793840527534485\n",
      "          vf_loss: 0.07766160136088729\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.1445385441184044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010077984887175262\n",
      "          policy_loss: 0.04497854021610692\n",
      "          total_loss: 0.1152477121213451\n",
      "          vf_explained_var: 0.783054769039154\n",
      "          vf_loss: 0.06006520916707814\n",
      "    num_agent_steps_sampled: 143186\n",
      "    num_agent_steps_trained: 143186\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.745849056603774\n",
      "    ram_util_percent: 62.64471698113207\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.105591982603073\n",
      "    player_1: 2.3165331035852432\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.03362832307815552\n",
      "    player_1: 0.02736240491271019\n",
      "  policy_reward_min:\n",
      "    player_0: -2.331428825855255\n",
      "    player_1: -3.0877497792243958\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0737050113841862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9714985018380324\n",
      "    mean_inference_ms: 4.104701389720917\n",
      "    mean_raw_obs_processing_ms: 0.24569088796790012\n",
      "  time_since_restore: 13651.103317022324\n",
      "  time_this_iter_s: 375.9275040626526\n",
      "  time_total_s: 13651.103317022324\n",
      "  timers:\n",
      "    learn_throughput: 10.694\n",
      "    learn_time_ms: 374055.84\n",
      "    sample_throughput: 890.036\n",
      "    sample_time_ms: 4494.203\n",
      "    update_time_ms: 19.069\n",
      "  timestamp: 1627301517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         13651.1</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-0.00626592</td><td style=\"text-align: right;\">            0.487658</td><td style=\"text-align: right;\">           -0.491138</td><td style=\"text-align: right;\">            166.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 147161\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-18-17\n",
      "  done: false\n",
      "  episode_len_mean: 166.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4876578599214554\n",
      "  episode_reward_mean: -0.0024272985756397247\n",
      "  episode_reward_min: -0.5140547454357147\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 896\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.0736010037362576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013637702912092209\n",
      "          policy_loss: 0.04017334966920316\n",
      "          total_loss: 0.12417029635980725\n",
      "          vf_explained_var: 0.6737267971038818\n",
      "          vf_loss: 0.07018877123482525\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.1333751901984215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013338835386093706\n",
      "          policy_loss: 0.055584995658136904\n",
      "          total_loss: 0.11677327868528664\n",
      "          vf_explained_var: 0.7705403566360474\n",
      "          vf_loss: 0.047682707430794835\n",
      "    num_agent_steps_sampled: 147161\n",
      "    num_agent_steps_trained: 147161\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.042616822429904\n",
      "    ram_util_percent: 62.02766355140187\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.105591982603073\n",
      "    player_1: 2.1196113526821136\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.033107195645570756\n",
      "    player_1: -0.03553449422121048\n",
      "  policy_reward_min:\n",
      "    player_0: -2.084081321954727\n",
      "    player_1: -3.0877497792243958\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07393162997153589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9712079443482585\n",
      "    mean_inference_ms: 4.10448315581186\n",
      "    mean_raw_obs_processing_ms: 0.24533420917463114\n",
      "  time_since_restore: 14030.954596042633\n",
      "  time_this_iter_s: 379.85127902030945\n",
      "  time_total_s: 14030.954596042633\n",
      "  timers:\n",
      "    learn_throughput: 10.694\n",
      "    learn_time_ms: 374038.863\n",
      "    sample_throughput: 890.92\n",
      "    sample_time_ms: 4489.742\n",
      "    update_time_ms: 19.262\n",
      "  timestamp: 1627301897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">           14031</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-0.0024273</td><td style=\"text-align: right;\">            0.487658</td><td style=\"text-align: right;\">           -0.514055</td><td style=\"text-align: right;\">            166.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 151139\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 166.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5088929831981659\n",
      "  episode_reward_mean: 0.0006784375756978989\n",
      "  episode_reward_min: -0.5140547454357147\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 918\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.1045069582760334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013104680692777038\n",
      "          policy_loss: 0.04967635066714138\n",
      "          total_loss: 0.24693943839520216\n",
      "          vf_explained_var: 0.4892632067203522\n",
      "          vf_loss: 0.18399460427463055\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.0885039567947388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010043327638413757\n",
      "          policy_loss: 0.03835132281528786\n",
      "          total_loss: 0.1481564991408959\n",
      "          vf_explained_var: 0.727491021156311\n",
      "          vf_loss: 0.09963630838319659\n",
      "    num_agent_steps_sampled: 151139\n",
      "    num_agent_steps_trained: 151139\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.995497185741087\n",
      "    ram_util_percent: 62.52476547842402\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.255053460597992\n",
      "    player_1: 3.1360086798667908\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.03748807944357395\n",
      "    player_1: 0.03816651701927185\n",
      "  policy_reward_min:\n",
      "    player_0: -3.123156614601612\n",
      "    player_1: -3.222152069211006\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07407801510934606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9707433488745157\n",
      "    mean_inference_ms: 4.103859143568506\n",
      "    mean_raw_obs_processing_ms: 0.2451832362873013\n",
      "  time_since_restore: 14409.775069952011\n",
      "  time_this_iter_s: 378.82047390937805\n",
      "  time_total_s: 14409.775069952011\n",
      "  timers:\n",
      "    learn_throughput: 10.699\n",
      "    learn_time_ms: 373877.353\n",
      "    sample_throughput: 890.157\n",
      "    sample_time_ms: 4493.589\n",
      "    update_time_ms: 19.822\n",
      "  timestamp: 1627302275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         14409.8</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">0.000678438</td><td style=\"text-align: right;\">            0.508893</td><td style=\"text-align: right;\">           -0.514055</td><td style=\"text-align: right;\">             166.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 155117\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-30-57\n",
      "  done: false\n",
      "  episode_len_mean: 168.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5088929831981659\n",
      "  episode_reward_mean: -0.0017667628824710846\n",
      "  episode_reward_min: -0.5140547454357147\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 941\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.141966387629509\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011462331749498844\n",
      "          policy_loss: 0.01982866565231234\n",
      "          total_loss: 0.11426447797566652\n",
      "          vf_explained_var: 0.5866849422454834\n",
      "          vf_loss: 0.0828302032314241\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.0807272642850876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013007449451833963\n",
      "          policy_loss: 0.04112527071265504\n",
      "          total_loss: 0.14484306355006993\n",
      "          vf_explained_var: 0.6918877959251404\n",
      "          vf_loss: 0.0905477530322969\n",
      "    num_agent_steps_sampled: 155117\n",
      "    num_agent_steps_trained: 155117\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.851396648044693\n",
      "    ram_util_percent: 62.98919925512104\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.255053460597992\n",
      "    player_1: 3.1360086798667908\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.017111390829086304\n",
      "    player_1: -0.018878153711557388\n",
      "  policy_reward_min:\n",
      "    player_0: -3.123156614601612\n",
      "    player_1: -3.222152069211006\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07450736216984705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9700236025042134\n",
      "    mean_inference_ms: 4.104187743913752\n",
      "    mean_raw_obs_processing_ms: 0.2452651693850286\n",
      "  time_since_restore: 14791.185204267502\n",
      "  time_this_iter_s: 381.4101343154907\n",
      "  time_total_s: 14791.185204267502\n",
      "  timers:\n",
      "    learn_throughput: 10.681\n",
      "    learn_time_ms: 374507.829\n",
      "    sample_throughput: 891.108\n",
      "    sample_time_ms: 4488.792\n",
      "    update_time_ms: 21.401\n",
      "  timestamp: 1627302657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         14791.2</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">-0.00176676</td><td style=\"text-align: right;\">            0.508893</td><td style=\"text-align: right;\">           -0.514055</td><td style=\"text-align: right;\">            168.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 159092\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-37-19\n",
      "  done: false\n",
      "  episode_len_mean: 168.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5088929831981659\n",
      "  episode_reward_mean: 0.0031669847667217255\n",
      "  episode_reward_min: -0.5140547454357147\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 966\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.0323207788169384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013862078660167754\n",
      "          policy_loss: 0.03198348206933588\n",
      "          total_loss: 0.1391957150772214\n",
      "          vf_explained_var: 0.5578508377075195\n",
      "          vf_loss: 0.0931768745649606\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.0858072601258755\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013089114625472575\n",
      "          policy_loss: 0.025308081472758204\n",
      "          total_loss: 0.1251809661043808\n",
      "          vf_explained_var: 0.6167696714401245\n",
      "          vf_loss: 0.08662015595473349\n",
      "    num_agent_steps_sampled: 159092\n",
      "    num_agent_steps_trained: 159092\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.810925925925925\n",
      "    ram_util_percent: 59.86222222222223\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.255053460597992\n",
      "    player_1: 3.1360086798667908\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.024289143830537797\n",
      "    player_1: 0.027456128597259523\n",
      "  policy_reward_min:\n",
      "    player_0: -3.123156614601612\n",
      "    player_1: -3.222152069211006\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07490377378413554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9694886952208015\n",
      "    mean_inference_ms: 4.103269329359811\n",
      "    mean_raw_obs_processing_ms: 0.24532164892168418\n",
      "  time_since_restore: 15173.6623980999\n",
      "  time_this_iter_s: 382.47719383239746\n",
      "  time_total_s: 15173.6623980999\n",
      "  timers:\n",
      "    learn_throughput: 10.672\n",
      "    learn_time_ms: 374808.231\n",
      "    sample_throughput: 890.225\n",
      "    sample_time_ms: 4493.244\n",
      "    update_time_ms: 21.885\n",
      "  timestamp: 1627303039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         15173.7</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">0.00316698</td><td style=\"text-align: right;\">            0.508893</td><td style=\"text-align: right;\">           -0.514055</td><td style=\"text-align: right;\">            168.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 163068\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-43-36\n",
      "  done: false\n",
      "  episode_len_mean: 169.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5088929831981659\n",
      "  episode_reward_mean: -0.0006911247968673706\n",
      "  episode_reward_min: -0.5140547454357147\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 990\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.992459699511528\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01531478192191571\n",
      "          policy_loss: 0.022847257205285132\n",
      "          total_loss: 0.10361387603916228\n",
      "          vf_explained_var: 0.47987571358680725\n",
      "          vf_loss: 0.06526040122844279\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.0802639499306679\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012929033720865846\n",
      "          policy_loss: 0.022943741350900382\n",
      "          total_loss: 0.08271998423151672\n",
      "          vf_explained_var: 0.7000320553779602\n",
      "          vf_loss: 0.04668559646233916\n",
      "    num_agent_steps_sampled: 163068\n",
      "    num_agent_steps_trained: 163068\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.844256120527305\n",
      "    ram_util_percent: 62.87250470809792\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.255053460597992\n",
      "    player_1: 3.1360086798667908\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.038034743666648864\n",
      "    player_1: -0.038725868463516236\n",
      "  policy_reward_min:\n",
      "    player_0: -3.123156614601612\n",
      "    player_1: -3.222152069211006\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07523709137526154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9694990618103736\n",
      "    mean_inference_ms: 4.1029030963784745\n",
      "    mean_raw_obs_processing_ms: 0.2452258131021655\n",
      "  time_since_restore: 15550.361909866333\n",
      "  time_this_iter_s: 376.6995117664337\n",
      "  time_total_s: 15550.361909866333\n",
      "  timers:\n",
      "    learn_throughput: 10.675\n",
      "    learn_time_ms: 374705.632\n",
      "    sample_throughput: 884.237\n",
      "    sample_time_ms: 4523.675\n",
      "    update_time_ms: 21.887\n",
      "  timestamp: 1627303416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         15550.4</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-0.000691125</td><td style=\"text-align: right;\">            0.508893</td><td style=\"text-align: right;\">           -0.514055</td><td style=\"text-align: right;\">            169.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 167046\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-49-57\n",
      "  done: false\n",
      "  episode_len_mean: 169.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5088929831981659\n",
      "  episode_reward_mean: 0.005337438359856605\n",
      "  episode_reward_min: -0.4646351933479309\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1012\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9807867407798767\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013527967967092991\n",
      "          policy_loss: 0.02068101440090686\n",
      "          total_loss: 0.08313906809780747\n",
      "          vf_explained_var: 0.48576194047927856\n",
      "          vf_loss: 0.04876098642125726\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9658728875219822\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014777241216506809\n",
      "          policy_loss: 0.012621232424862683\n",
      "          total_loss: 0.06711630045901984\n",
      "          vf_explained_var: 0.6374068260192871\n",
      "          vf_loss: 0.0395331111503765\n",
      "    num_agent_steps_sampled: 167046\n",
      "    num_agent_steps_trained: 167046\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.19365671641791\n",
      "    ram_util_percent: 61.42593283582089\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.0445148795843124\n",
      "    player_1: 1.7765721678733826\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.038462112322449685\n",
      "    player_1: -0.03312467396259308\n",
      "  policy_reward_min:\n",
      "    player_0: -2.2412073612213135\n",
      "    player_1: -3.0367617309093475\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07536705998710874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9696575785706565\n",
      "    mean_inference_ms: 4.103593636556676\n",
      "    mean_raw_obs_processing_ms: 0.24517420449129265\n",
      "  time_since_restore: 15931.596864700317\n",
      "  time_this_iter_s: 381.2349548339844\n",
      "  time_total_s: 15931.596864700317\n",
      "  timers:\n",
      "    learn_throughput: 10.677\n",
      "    learn_time_ms: 374646.736\n",
      "    sample_throughput: 883.975\n",
      "    sample_time_ms: 4525.017\n",
      "    update_time_ms: 20.583\n",
      "  timestamp: 1627303797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         15931.6</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">0.00533744</td><td style=\"text-align: right;\">            0.508893</td><td style=\"text-align: right;\">           -0.464635</td><td style=\"text-align: right;\">            169.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 171023\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_05-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 168.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.47081321477890015\n",
      "  episode_reward_mean: 0.0018887445330619812\n",
      "  episode_reward_min: -0.4646351933479309\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1036\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9998495131731033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011447015218436718\n",
      "          policy_loss: 0.030648142332211137\n",
      "          total_loss: 0.1217607983853668\n",
      "          vf_explained_var: 0.5637997388839722\n",
      "          vf_loss: 0.07952255220152438\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9541231766343117\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012629930162802339\n",
      "          policy_loss: 0.023598220839630812\n",
      "          total_loss: 0.09413872344885021\n",
      "          vf_explained_var: 0.6965905427932739\n",
      "          vf_loss: 0.05775269842706621\n",
      "    num_agent_steps_sampled: 171023\n",
      "    num_agent_steps_trained: 171023\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.171401869158878\n",
      "    ram_util_percent: 62.50448598130841\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.0445148795843124\n",
      "    player_1: 1.9610066711902618\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.004315881431102753\n",
      "    player_1: 0.006204625964164734\n",
      "  policy_reward_min:\n",
      "    player_0: -2.2412073612213135\n",
      "    player_1: -3.0367617309093475\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07531393338468775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9688720994913891\n",
      "    mean_inference_ms: 4.104033052974933\n",
      "    mean_raw_obs_processing_ms: 0.24504200105704746\n",
      "  time_since_restore: 16311.235715389252\n",
      "  time_this_iter_s: 379.6388506889343\n",
      "  time_total_s: 16311.235715389252\n",
      "  timers:\n",
      "    learn_throughput: 10.681\n",
      "    learn_time_ms: 374496.286\n",
      "    sample_throughput: 883.369\n",
      "    sample_time_ms: 4528.121\n",
      "    update_time_ms: 21.729\n",
      "  timestamp: 1627304177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         16311.2</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">0.00188874</td><td style=\"text-align: right;\">            0.470813</td><td style=\"text-align: right;\">           -0.464635</td><td style=\"text-align: right;\">            168.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 174998\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-02-39\n",
      "  done: false\n",
      "  episode_len_mean: 169.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.351642370223999\n",
      "  episode_reward_mean: 0.0030019260942935944\n",
      "  episode_reward_min: -0.30365467071533203\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1061\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9518574588000774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011318057688185945\n",
      "          policy_loss: 0.023048970964737236\n",
      "          total_loss: 0.12699179304763675\n",
      "          vf_explained_var: 0.4920194149017334\n",
      "          vf_loss: 0.09248329093679786\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9628242626786232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011010505957528949\n",
      "          policy_loss: 0.021118203352671117\n",
      "          total_loss: 0.08723941422067583\n",
      "          vf_explained_var: 0.7708631753921509\n",
      "          vf_loss: 0.05497307376936078\n",
      "    num_agent_steps_sampled: 174998\n",
      "    num_agent_steps_trained: 174998\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.0364312267658\n",
      "    ram_util_percent: 62.8581784386617\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.0445148795843124\n",
      "    player_1: 2.098221406340599\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.01661426141858101\n",
      "    player_1: -0.013612335324287414\n",
      "  policy_reward_min:\n",
      "    player_0: -2.0606909692287445\n",
      "    player_1: -3.0367617309093475\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07516076829213093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9682521788305436\n",
      "    mean_inference_ms: 4.105311664588982\n",
      "    mean_raw_obs_processing_ms: 0.24462812489033367\n",
      "  time_since_restore: 16693.10130262375\n",
      "  time_this_iter_s: 381.86558723449707\n",
      "  time_total_s: 16693.10130262375\n",
      "  timers:\n",
      "    learn_throughput: 10.66\n",
      "    learn_time_ms: 375221.392\n",
      "    sample_throughput: 883.716\n",
      "    sample_time_ms: 4526.342\n",
      "    update_time_ms: 22.984\n",
      "  timestamp: 1627304559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         16693.1</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">0.00300193</td><td style=\"text-align: right;\">            0.351642</td><td style=\"text-align: right;\">           -0.303655</td><td style=\"text-align: right;\">            169.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 178976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 168.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.21775811910629272\n",
      "  episode_reward_mean: 0.00040539950132369997\n",
      "  episode_reward_min: -0.203237384557724\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1084\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9204311370849609\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01669340004445985\n",
      "          policy_loss: 0.02426050551002845\n",
      "          total_loss: 0.11532915779389441\n",
      "          vf_explained_var: 0.6488839387893677\n",
      "          vf_loss: 0.07416658289730549\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8861248381435871\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01159212802303955\n",
      "          policy_loss: 0.031191607791697606\n",
      "          total_loss: 0.0865773509722203\n",
      "          vf_explained_var: 0.7917510867118835\n",
      "          vf_loss: 0.04364871419966221\n",
      "    num_agent_steps_sampled: 178976\n",
      "    num_agent_steps_trained: 178976\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.11074074074074\n",
      "    ram_util_percent: 60.9724074074074\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.025243192911148\n",
      "    player_1: 2.098221406340599\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.010446829795837402\n",
      "    player_1: 0.010852229297161103\n",
      "  policy_reward_min:\n",
      "    player_0: -2.0606909692287445\n",
      "    player_1: -2.0053714513778687\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07517923508429536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9680723121766888\n",
      "    mean_inference_ms: 4.106476553976\n",
      "    mean_raw_obs_processing_ms: 0.2440825317223074\n",
      "  time_since_restore: 17076.22690677643\n",
      "  time_this_iter_s: 383.12560415267944\n",
      "  time_total_s: 17076.22690677643\n",
      "  timers:\n",
      "    learn_throughput: 10.652\n",
      "    learn_time_ms: 375498.802\n",
      "    sample_throughput: 882.445\n",
      "    sample_time_ms: 4532.859\n",
      "    update_time_ms: 23.468\n",
      "  timestamp: 1627304942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         17076.2</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">0.0004054</td><td style=\"text-align: right;\">            0.217758</td><td style=\"text-align: right;\">           -0.203237</td><td style=\"text-align: right;\">            168.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 182953\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 167.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.21775811910629272\n",
      "  episode_reward_mean: -0.0020721897482872008\n",
      "  episode_reward_min: -0.203237384557724\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1108\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8943781331181526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013129546015989035\n",
      "          policy_loss: 0.03177569224499166\n",
      "          total_loss: 0.11319200514117256\n",
      "          vf_explained_var: 0.5079443454742432\n",
      "          vf_loss: 0.06812264793552458\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8915238231420517\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012474434566684067\n",
      "          policy_loss: 0.020304415200371295\n",
      "          total_loss: 0.08017086307518184\n",
      "          vf_explained_var: 0.7619373202323914\n",
      "          vf_loss: 0.04723608447238803\n",
      "    num_agent_steps_sampled: 182953\n",
      "    num_agent_steps_trained: 182953\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.85139146567718\n",
      "    ram_util_percent: 63.36011131725416\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.3071107864379883\n",
      "    player_1: 2.098221406340599\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0028473100066184996\n",
      "    player_1: 0.0007751202583312989\n",
      "  policy_reward_min:\n",
      "    player_0: -2.0606909692287445\n",
      "    player_1: -2.3090288639068604\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07560545171380062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9683201251339517\n",
      "    mean_inference_ms: 4.1058312362754945\n",
      "    mean_raw_obs_processing_ms: 0.24372645216296043\n",
      "  time_since_restore: 17458.962167024612\n",
      "  time_this_iter_s: 382.7352602481842\n",
      "  time_total_s: 17458.962167024612\n",
      "  timers:\n",
      "    learn_throughput: 10.633\n",
      "    learn_time_ms: 376188.666\n",
      "    sample_throughput: 883.701\n",
      "    sample_time_ms: 4526.416\n",
      "    update_time_ms: 21.906\n",
      "  timestamp: 1627305325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">           17459</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-0.00207219</td><td style=\"text-align: right;\">            0.217758</td><td style=\"text-align: right;\">           -0.203237</td><td style=\"text-align: right;\">            167.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 186930\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-21-48\n",
      "  done: false\n",
      "  episode_len_mean: 166.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.21775811910629272\n",
      "  episode_reward_mean: -0.0025256164371967316\n",
      "  episode_reward_min: -0.2708292007446289\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1133\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.9123660288751125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01145681960042566\n",
      "          policy_loss: 0.022642184747383\n",
      "          total_loss: 0.12571207096334547\n",
      "          vf_explained_var: 0.549735426902771\n",
      "          vf_loss: 0.09146985318511724\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.894768264144659\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011367542669177055\n",
      "          policy_loss: 0.0280905258259736\n",
      "          total_loss: 0.16238998668268323\n",
      "          vf_explained_var: 0.524966299533844\n",
      "          vf_loss: 0.12278982205316424\n",
      "    num_agent_steps_sampled: 186930\n",
      "    num_agent_steps_trained: 186930\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.244341372912801\n",
      "    ram_util_percent: 61.23265306122449\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.3071107864379883\n",
      "    player_1: 2.098221406340599\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.016201302409172058\n",
      "    player_1: 0.013675685971975327\n",
      "  policy_reward_min:\n",
      "    player_0: -2.0606909692287445\n",
      "    player_1: -2.3090288639068604\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07608023811490786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9696119392600806\n",
      "    mean_inference_ms: 4.103674172506384\n",
      "    mean_raw_obs_processing_ms: 0.24336335431564302\n",
      "  time_since_restore: 17841.917551755905\n",
      "  time_this_iter_s: 382.9553847312927\n",
      "  time_total_s: 17841.917551755905\n",
      "  timers:\n",
      "    learn_throughput: 10.624\n",
      "    learn_time_ms: 376498.581\n",
      "    sample_throughput: 883.327\n",
      "    sample_time_ms: 4528.332\n",
      "    update_time_ms: 21.897\n",
      "  timestamp: 1627305708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         17841.9</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">-0.00252562</td><td style=\"text-align: right;\">            0.217758</td><td style=\"text-align: right;\">           -0.270829</td><td style=\"text-align: right;\">            166.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 190906\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-28-08\n",
      "  done: false\n",
      "  episode_len_mean: 166.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2790452539920807\n",
      "  episode_reward_mean: -0.0011309921741485597\n",
      "  episode_reward_min: -0.2708292007446289\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1157\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8626994080841541\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011026714084437117\n",
      "          policy_loss: 0.016470959526486695\n",
      "          total_loss: 0.1895892722532153\n",
      "          vf_explained_var: 0.4669538140296936\n",
      "          vf_loss: 0.16195375844836235\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8513973131775856\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010732891911175102\n",
      "          policy_loss: 0.016203689883695915\n",
      "          total_loss: 0.1520032095722854\n",
      "          vf_explained_var: 0.646145761013031\n",
      "          vf_loss: 0.12493246328085661\n",
      "    num_agent_steps_sampled: 190906\n",
      "    num_agent_steps_trained: 190906\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.985981308411215\n",
      "    ram_util_percent: 63.609158878504665\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.738456055521965\n",
      "    player_1: 2.6026052981615067\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.011158046126365662\n",
      "    player_1: 0.010027053952217101\n",
      "  policy_reward_min:\n",
      "    player_0: -2.6418234407901764\n",
      "    player_1: -2.7303098142147064\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07667709005837589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9712411325392959\n",
      "    mean_inference_ms: 4.100815692289922\n",
      "    mean_raw_obs_processing_ms: 0.24351912869037776\n",
      "  time_since_restore: 18221.415287971497\n",
      "  time_this_iter_s: 379.49773621559143\n",
      "  time_total_s: 18221.415287971497\n",
      "  timers:\n",
      "    learn_throughput: 10.622\n",
      "    learn_time_ms: 376559.338\n",
      "    sample_throughput: 881.644\n",
      "    sample_time_ms: 4536.979\n",
      "    update_time_ms: 23.146\n",
      "  timestamp: 1627306088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         18221.4</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-0.00113099</td><td style=\"text-align: right;\">            0.279045</td><td style=\"text-align: right;\">           -0.270829</td><td style=\"text-align: right;\">            166.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 194885\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-34-31\n",
      "  done: false\n",
      "  episode_len_mean: 165.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2790452539920807\n",
      "  episode_reward_mean: 0.0008966825902462005\n",
      "  episode_reward_min: -0.2708292007446289\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1182\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7928293459117413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010642024804838002\n",
      "          policy_loss: 0.023041911306791008\n",
      "          total_loss: 0.09499839460477233\n",
      "          vf_explained_var: 0.7170650362968445\n",
      "          vf_loss: 0.06118143396452069\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8883827850222588\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010412352887215093\n",
      "          policy_loss: 0.021212786785326898\n",
      "          total_loss: 0.10269036644604057\n",
      "          vf_explained_var: 0.6951529383659363\n",
      "          vf_loss: 0.07093507540412247\n",
      "    num_agent_steps_sampled: 194885\n",
      "    num_agent_steps_trained: 194885\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.162708719851576\n",
      "    ram_util_percent: 62.54081632653061\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.738456055521965\n",
      "    player_1: 2.6026052981615067\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.051449213027954105\n",
      "    player_1: 0.0523458956182003\n",
      "  policy_reward_min:\n",
      "    player_0: -2.6418234407901764\n",
      "    player_1: -2.7303098142147064\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07732356882835718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9723849855640125\n",
      "    mean_inference_ms: 4.098825783645265\n",
      "    mean_raw_obs_processing_ms: 0.24353619210907185\n",
      "  time_since_restore: 18604.22792696953\n",
      "  time_this_iter_s: 382.8126389980316\n",
      "  time_total_s: 18604.22792696953\n",
      "  timers:\n",
      "    learn_throughput: 10.618\n",
      "    learn_time_ms: 376701.248\n",
      "    sample_throughput: 881.754\n",
      "    sample_time_ms: 4536.414\n",
      "    update_time_ms: 22.323\n",
      "  timestamp: 1627306471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         18604.2</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">0.000896683</td><td style=\"text-align: right;\">            0.279045</td><td style=\"text-align: right;\">           -0.270829</td><td style=\"text-align: right;\">             165.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 198862\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-40-50\n",
      "  done: false\n",
      "  episode_len_mean: 166.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7561546564102173\n",
      "  episode_reward_mean: 0.0008176550269126892\n",
      "  episode_reward_min: -0.749002993106842\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1204\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8343536593019962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013209480210207403\n",
      "          policy_loss: 0.012258114758878946\n",
      "          total_loss: 0.07539266342064366\n",
      "          vf_explained_var: 0.5383814573287964\n",
      "          vf_loss: 0.0497599458321929\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.84715611115098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011254503042437136\n",
      "          policy_loss: 0.021300426335074008\n",
      "          total_loss: 0.08509372087428346\n",
      "          vf_explained_var: 0.6794652938842773\n",
      "          vf_loss: 0.05239811027422547\n",
      "    num_agent_steps_sampled: 198862\n",
      "    num_agent_steps_trained: 198862\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.156448598130842\n",
      "    ram_util_percent: 63.33345794392524\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.738456055521965\n",
      "    player_1: 2.6026052981615067\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.033163214921951296\n",
      "    player_1: 0.033980869948863984\n",
      "  policy_reward_min:\n",
      "    player_0: -2.6418234407901764\n",
      "    player_1: -2.7303098142147064\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.077579580835973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9718956302059197\n",
      "    mean_inference_ms: 4.097765265690614\n",
      "    mean_raw_obs_processing_ms: 0.2438607228394986\n",
      "  time_since_restore: 18983.636187553406\n",
      "  time_this_iter_s: 379.40826058387756\n",
      "  time_total_s: 18983.636187553406\n",
      "  timers:\n",
      "    learn_throughput: 10.627\n",
      "    learn_time_ms: 376392.544\n",
      "    sample_throughput: 881.62\n",
      "    sample_time_ms: 4537.104\n",
      "    update_time_ms: 22.088\n",
      "  timestamp: 1627306850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         18983.6</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">0.000817655</td><td style=\"text-align: right;\">            0.756155</td><td style=\"text-align: right;\">           -0.749003</td><td style=\"text-align: right;\">            166.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 202838\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-47-12\n",
      "  done: false\n",
      "  episode_len_mean: 168.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7561546564102173\n",
      "  episode_reward_mean: 0.003053615987300873\n",
      "  episode_reward_min: -0.749002993106842\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1228\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8047435916960239\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010866865864954889\n",
      "          policy_loss: 0.012966892449185252\n",
      "          total_loss: 0.11881432705558836\n",
      "          vf_explained_var: 0.6061452627182007\n",
      "          vf_loss: 0.09484473639167845\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8180316612124443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012567421945277601\n",
      "          policy_loss: 0.03209500713273883\n",
      "          total_loss: 0.12445654405746609\n",
      "          vf_explained_var: 0.633647084236145\n",
      "          vf_loss: 0.07963701756671071\n",
      "    num_agent_steps_sampled: 202838\n",
      "    num_agent_steps_trained: 202838\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.99366852886406\n",
      "    ram_util_percent: 63.614525139664806\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.738456055521965\n",
      "    player_1: 2.6026052981615067\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.047300146222114564\n",
      "    player_1: 0.05035376220941543\n",
      "  policy_reward_min:\n",
      "    player_0: -2.6418234407901764\n",
      "    player_1: -2.7303098142147064\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07785426621042749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9715618295179447\n",
      "    mean_inference_ms: 4.097467281018542\n",
      "    mean_raw_obs_processing_ms: 0.24399760977101057\n",
      "  time_since_restore: 19365.44029569626\n",
      "  time_this_iter_s: 381.8041081428528\n",
      "  time_total_s: 19365.44029569626\n",
      "  timers:\n",
      "    learn_throughput: 10.612\n",
      "    learn_time_ms: 376938.732\n",
      "    sample_throughput: 888.682\n",
      "    sample_time_ms: 4501.048\n",
      "    update_time_ms: 22.087\n",
      "  timestamp: 1627307232\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         19365.4</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">0.00305362</td><td style=\"text-align: right;\">            0.756155</td><td style=\"text-align: right;\">           -0.749003</td><td style=\"text-align: right;\">            168.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 206814\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-53-35\n",
      "  done: false\n",
      "  episode_len_mean: 168.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7561546564102173\n",
      "  episode_reward_mean: -0.002671980857849121\n",
      "  episode_reward_min: -0.749002993106842\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1252\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7724885381758213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01476651115808636\n",
      "          policy_loss: 0.01585260676802136\n",
      "          total_loss: 0.11491768469568342\n",
      "          vf_explained_var: 0.6820502281188965\n",
      "          vf_loss: 0.08411398786120117\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8016634359955788\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012918444001115859\n",
      "          policy_loss: 0.006189268664456904\n",
      "          total_loss: 0.10797809308860451\n",
      "          vf_explained_var: 0.651337206363678\n",
      "          vf_loss: 0.08870890364050865\n",
      "    num_agent_steps_sampled: 206814\n",
      "    num_agent_steps_trained: 206814\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.445925925925927\n",
      "    ram_util_percent: 60.3625925925926\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.738456055521965\n",
      "    player_1: 2.6026052981615067\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.0011281180381774903\n",
      "    player_1: -0.0038000988960266114\n",
      "  policy_reward_min:\n",
      "    player_0: -2.6418234407901764\n",
      "    player_1: -2.7303098142147064\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07788513726910742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9711311089701082\n",
      "    mean_inference_ms: 4.0976188724473666\n",
      "    mean_raw_obs_processing_ms: 0.24395742839602047\n",
      "  time_since_restore: 19748.397297859192\n",
      "  time_this_iter_s: 382.95700216293335\n",
      "  time_total_s: 19748.397297859192\n",
      "  timers:\n",
      "    learn_throughput: 10.607\n",
      "    learn_time_ms: 377112.349\n",
      "    sample_throughput: 888.919\n",
      "    sample_time_ms: 4499.845\n",
      "    update_time_ms: 23.629\n",
      "  timestamp: 1627307615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         19748.4</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">-0.00267198</td><td style=\"text-align: right;\">            0.756155</td><td style=\"text-align: right;\">           -0.749003</td><td style=\"text-align: right;\">            168.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 210792\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_06-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 169.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7561546564102173\n",
      "  episode_reward_mean: 0.0019376151263713836\n",
      "  episode_reward_min: -0.749002993106842\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1274\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7853282205760479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015266374859493226\n",
      "          policy_loss: 0.005552583723329008\n",
      "          total_loss: 0.0908972134348005\n",
      "          vf_explained_var: 0.6873229742050171\n",
      "          vf_loss: 0.06988742365501821\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7428630143404007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010127937683137134\n",
      "          policy_loss: 0.005741359782405198\n",
      "          total_loss: 0.08434993540868163\n",
      "          vf_explained_var: 0.7226470708847046\n",
      "          vf_loss: 0.0683540387544781\n",
      "    num_agent_steps_sampled: 210792\n",
      "    num_agent_steps_trained: 210792\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.255283018867923\n",
      "    ram_util_percent: 63.86075471698113\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.7172187715768814\n",
      "    player_1: 2.1307440102100372\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0032604394853115084\n",
      "    player_1: 0.0051980546116828915\n",
      "  policy_reward_min:\n",
      "    player_0: -2.140226811170578\n",
      "    player_1: -2.7300196141004562\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07797861487715345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.971147868403597\n",
      "    mean_inference_ms: 4.098120297898868\n",
      "    mean_raw_obs_processing_ms: 0.24364022308784467\n",
      "  time_since_restore: 20125.330883026123\n",
      "  time_this_iter_s: 376.93358516693115\n",
      "  time_total_s: 20125.330883026123\n",
      "  timers:\n",
      "    learn_throughput: 10.615\n",
      "    learn_time_ms: 376838.979\n",
      "    sample_throughput: 888.011\n",
      "    sample_time_ms: 4504.449\n",
      "    update_time_ms: 22.673\n",
      "  timestamp: 1627307992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         20125.3</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">0.00193762</td><td style=\"text-align: right;\">            0.756155</td><td style=\"text-align: right;\">           -0.749003</td><td style=\"text-align: right;\">            169.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 214768\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 170.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.42210251092910767\n",
      "  episode_reward_mean: 0.000954851508140564\n",
      "  episode_reward_min: -0.3980053961277008\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1298\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.8019839338958263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011432848230469972\n",
      "          policy_loss: 0.026048823492601514\n",
      "          total_loss: 0.11072681401856244\n",
      "          vf_explained_var: 0.6535382270812988\n",
      "          vf_loss: 0.07310223486274481\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7741184309124947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009782070817891508\n",
      "          policy_loss: 0.010793820722028613\n",
      "          total_loss: 0.09919562935829163\n",
      "          vf_explained_var: 0.6998406648635864\n",
      "          vf_loss: 0.07849745685234666\n",
      "    num_agent_steps_sampled: 214768\n",
      "    num_agent_steps_trained: 214768\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.558736059479555\n",
      "    ram_util_percent: 61.41394052044609\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.7172187715768814\n",
      "    player_1: 2.546958141028881\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.01223292201757431\n",
      "    player_1: -0.011278070509433746\n",
      "  policy_reward_min:\n",
      "    player_0: -2.507849633693695\n",
      "    player_1: -2.7300196141004562\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07780507437353884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9710487171540487\n",
      "    mean_inference_ms: 4.098472514953445\n",
      "    mean_raw_obs_processing_ms: 0.2433249725285524\n",
      "  time_since_restore: 20507.21311378479\n",
      "  time_this_iter_s: 381.882230758667\n",
      "  time_total_s: 20507.21311378479\n",
      "  timers:\n",
      "    learn_throughput: 10.614\n",
      "    learn_time_ms: 376844.693\n",
      "    sample_throughput: 888.408\n",
      "    sample_time_ms: 4502.435\n",
      "    update_time_ms: 21.608\n",
      "  timestamp: 1627308374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         20507.2</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">0.000954852</td><td style=\"text-align: right;\">            0.422103</td><td style=\"text-align: right;\">           -0.398005</td><td style=\"text-align: right;\">               170</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 218744\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 168.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.42210251092910767\n",
      "  episode_reward_mean: -0.00042228251695632937\n",
      "  episode_reward_min: -0.3980053961277008\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1324\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7783854156732559\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01202682062285021\n",
      "          policy_loss: 0.012889442790765315\n",
      "          total_loss: 0.08924112690147012\n",
      "          vf_explained_var: 0.6701078414916992\n",
      "          vf_loss: 0.06417452543973923\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7539195865392685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010359240317484364\n",
      "          policy_loss: 0.01008062984328717\n",
      "          total_loss: 0.07653573551215231\n",
      "          vf_explained_var: 0.7510764598846436\n",
      "          vf_loss: 0.055966375628486276\n",
      "    num_agent_steps_sampled: 218744\n",
      "    num_agent_steps_trained: 218744\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.169981238273921\n",
      "    ram_util_percent: 63.37748592870543\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.7172187715768814\n",
      "    player_1: 2.546958141028881\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.005693474113941193\n",
      "    player_1: 0.005271191596984863\n",
      "  policy_reward_min:\n",
      "    player_0: -2.507849633693695\n",
      "    player_1: -2.7300196141004562\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07770405059258184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.970641427302207\n",
      "    mean_inference_ms: 4.0989185318356975\n",
      "    mean_raw_obs_processing_ms: 0.24322117097516613\n",
      "  time_since_restore: 20886.42051434517\n",
      "  time_this_iter_s: 379.207400560379\n",
      "  time_total_s: 20886.42051434517\n",
      "  timers:\n",
      "    learn_throughput: 10.626\n",
      "    learn_time_ms: 376449.376\n",
      "    sample_throughput: 888.26\n",
      "    sample_time_ms: 4503.184\n",
      "    update_time_ms: 20.867\n",
      "  timestamp: 1627308753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         20886.4</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-0.000422283</td><td style=\"text-align: right;\">            0.422103</td><td style=\"text-align: right;\">           -0.398005</td><td style=\"text-align: right;\">            168.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 222723\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-18-54\n",
      "  done: false\n",
      "  episode_len_mean: 168.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.42210251092910767\n",
      "  episode_reward_mean: -5.179047584533691e-05\n",
      "  episode_reward_min: -0.37687331438064575\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1345\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.79128348082304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012285807228181511\n",
      "          policy_loss: 0.01520316704409197\n",
      "          total_loss: 0.08294719422701746\n",
      "          vf_explained_var: 0.7132752537727356\n",
      "          vf_loss: 0.05530464625917375\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7364278435707092\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010861613671295345\n",
      "          policy_loss: 0.01574266151874326\n",
      "          total_loss: 0.07698005274869502\n",
      "          vf_explained_var: 0.7893174290657043\n",
      "          vf_loss: 0.05024000769481063\n",
      "    num_agent_steps_sampled: 222723\n",
      "    num_agent_steps_trained: 222723\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.300932835820895\n",
      "    ram_util_percent: 63.34384328358208\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.7172187715768814\n",
      "    player_1: 2.546958141028881\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.04184461951255798\n",
      "    player_1: 0.04179282903671264\n",
      "  policy_reward_min:\n",
      "    player_0: -2.507849633693695\n",
      "    player_1: -2.7300196141004562\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07790697737089909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9704931550811251\n",
      "    mean_inference_ms: 4.099608688650023\n",
      "    mean_raw_obs_processing_ms: 0.24302208432644115\n",
      "  time_since_restore: 21267.138766527176\n",
      "  time_this_iter_s: 380.71825218200684\n",
      "  time_total_s: 21267.138766527176\n",
      "  timers:\n",
      "    learn_throughput: 10.631\n",
      "    learn_time_ms: 376240.741\n",
      "    sample_throughput: 887.522\n",
      "    sample_time_ms: 4506.933\n",
      "    update_time_ms: 22.43\n",
      "  timestamp: 1627309134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         21267.1</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-5.17905e-05</td><td style=\"text-align: right;\">            0.422103</td><td style=\"text-align: right;\">           -0.376873</td><td style=\"text-align: right;\">            168.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 226699\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 167.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39110325276851654\n",
      "  episode_reward_mean: -0.0005258254706859589\n",
      "  episode_reward_min: -0.37687331438064575\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1370\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7307028062641621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011815391306299716\n",
      "          policy_loss: 0.009172684338409454\n",
      "          total_loss: 0.12506542494520545\n",
      "          vf_explained_var: 0.6861726641654968\n",
      "          vf_loss: 0.10392966168001294\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7196305841207504\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016445343499071896\n",
      "          policy_loss: 0.016517661104444414\n",
      "          total_loss: 0.12007766216993332\n",
      "          vf_explained_var: 0.7576582431793213\n",
      "          vf_loss: 0.0869090836495161\n",
      "    num_agent_steps_sampled: 226699\n",
      "    num_agent_steps_trained: 226699\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.195327102803738\n",
      "    ram_util_percent: 63.39401869158879\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.7172187715768814\n",
      "    player_1: 2.546958141028881\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.008051599413156509\n",
      "    player_1: 0.007525773942470551\n",
      "  policy_reward_min:\n",
      "    player_0: -2.507849633693695\n",
      "    player_1: -2.7300196141004562\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07785219506914628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9702492129182195\n",
      "    mean_inference_ms: 4.099884501971114\n",
      "    mean_raw_obs_processing_ms: 0.24287172431227738\n",
      "  time_since_restore: 21646.9380671978\n",
      "  time_this_iter_s: 379.7993006706238\n",
      "  time_total_s: 21646.9380671978\n",
      "  timers:\n",
      "    learn_throughput: 10.641\n",
      "    learn_time_ms: 375917.46\n",
      "    sample_throughput: 886.679\n",
      "    sample_time_ms: 4511.217\n",
      "    update_time_ms: 22.457\n",
      "  timestamp: 1627309514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         21646.9</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-0.000525825</td><td style=\"text-align: right;\">            0.391103</td><td style=\"text-align: right;\">           -0.376873</td><td style=\"text-align: right;\">            167.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 230676\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-31-36\n",
      "  done: false\n",
      "  episode_len_mean: 167.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39110325276851654\n",
      "  episode_reward_mean: -0.0004936486482620239\n",
      "  episode_reward_min: -0.37687331438064575\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1394\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7164573222398758\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01409184094518423\n",
      "          policy_loss: 0.02222554065519944\n",
      "          total_loss: 0.15146972611546516\n",
      "          vf_explained_var: 0.49301302433013916\n",
      "          vf_loss: 0.11497619515284896\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7321432121098042\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014012960018590093\n",
      "          policy_loss: 0.02428912284085527\n",
      "          total_loss: 0.11151531845098361\n",
      "          vf_explained_var: 0.7354645133018494\n",
      "          vf_loss: 0.0730380758177489\n",
      "    num_agent_steps_sampled: 230676\n",
      "    num_agent_steps_trained: 230676\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.130912476722532\n",
      "    ram_util_percent: 63.83054003724395\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.394713908433914\n",
      "    player_1: 2.116335391998291\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.024311004281044005\n",
      "    player_1: 0.02381735563278198\n",
      "  policy_reward_min:\n",
      "    player_0: -2.121591478586197\n",
      "    player_1: -2.42927148938179\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07788424759996689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9699225266174715\n",
      "    mean_inference_ms: 4.099885162210908\n",
      "    mean_raw_obs_processing_ms: 0.24290040139872537\n",
      "  time_since_restore: 22029.053711414337\n",
      "  time_this_iter_s: 382.1156442165375\n",
      "  time_total_s: 22029.053711414337\n",
      "  timers:\n",
      "    learn_throughput: 10.633\n",
      "    learn_time_ms: 376188.081\n",
      "    sample_throughput: 888.559\n",
      "    sample_time_ms: 4501.672\n",
      "    update_time_ms: 22.774\n",
      "  timestamp: 1627309896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         22029.1</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-0.000493649</td><td style=\"text-align: right;\">            0.391103</td><td style=\"text-align: right;\">           -0.376873</td><td style=\"text-align: right;\">            167.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 234652\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 167.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39110325276851654\n",
      "  episode_reward_mean: 0.002648817002773285\n",
      "  episode_reward_min: -0.37687331438064575\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1419\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7396693900227547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012359951884718612\n",
      "          policy_loss: 0.014976738835684955\n",
      "          total_loss: 0.09365009737666696\n",
      "          vf_explained_var: 0.7175599932670593\n",
      "          vf_loss: 0.0661589065566659\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7828399203717709\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010678107559215277\n",
      "          policy_loss: 0.017746694560628384\n",
      "          total_loss: 0.08683827728964388\n",
      "          vf_explained_var: 0.7978366613388062\n",
      "          vf_loss: 0.05827999929897487\n",
      "    num_agent_steps_sampled: 234652\n",
      "    num_agent_steps_trained: 234652\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.31611111111111\n",
      "    ram_util_percent: 60.99333333333335\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.394713908433914\n",
      "    player_1: 2.0992499589920044\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.026637750566005706\n",
      "    player_1: 0.029286567568778992\n",
      "  policy_reward_min:\n",
      "    player_0: -2.0886991918087006\n",
      "    player_1: -2.42927148938179\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07792371773946057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.970269051050081\n",
      "    mean_inference_ms: 4.099731101629752\n",
      "    mean_raw_obs_processing_ms: 0.24278828652687984\n",
      "  time_since_restore: 22412.4661591053\n",
      "  time_this_iter_s: 383.41244769096375\n",
      "  time_total_s: 22412.4661591053\n",
      "  timers:\n",
      "    learn_throughput: 10.631\n",
      "    learn_time_ms: 376246.511\n",
      "    sample_throughput: 887.914\n",
      "    sample_time_ms: 4504.943\n",
      "    update_time_ms: 22.286\n",
      "  timestamp: 1627310279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         22412.5</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">0.00264882</td><td style=\"text-align: right;\">            0.391103</td><td style=\"text-align: right;\">           -0.376873</td><td style=\"text-align: right;\">            167.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 238631\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 167.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39110325276851654\n",
      "  episode_reward_mean: 0.0027048207819461824\n",
      "  episode_reward_min: -0.3792615979909897\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1441\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6525150425732136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012785671569872648\n",
      "          policy_loss: 0.006887557567097247\n",
      "          total_loss: 0.13999388506636024\n",
      "          vf_explained_var: 0.6382715702056885\n",
      "          vf_loss: 0.12016083672642708\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7800748199224472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013224717346020043\n",
      "          policy_loss: 0.009120378876104951\n",
      "          total_loss: 0.125678274926031\n",
      "          vf_explained_var: 0.7439526319503784\n",
      "          vf_loss: 0.10316786682233214\n",
      "    num_agent_steps_sampled: 238631\n",
      "    num_agent_steps_trained: 238631\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.316885553470918\n",
      "    ram_util_percent: 64.2499061913696\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.394713908433914\n",
      "    player_1: 2.0992499589920044\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.03739308118820191\n",
      "    player_1: -0.03468826040625572\n",
      "  policy_reward_min:\n",
      "    player_0: -2.0886991918087006\n",
      "    player_1: -2.42927148938179\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07790712991412012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.971027521178786\n",
      "    mean_inference_ms: 4.0996503603217445\n",
      "    mean_raw_obs_processing_ms: 0.24253534983882852\n",
      "  time_since_restore: 22791.061953783035\n",
      "  time_this_iter_s: 378.5957946777344\n",
      "  time_total_s: 22791.061953783035\n",
      "  timers:\n",
      "    learn_throughput: 10.634\n",
      "    learn_time_ms: 376149.503\n",
      "    sample_throughput: 884.673\n",
      "    sample_time_ms: 4521.444\n",
      "    update_time_ms: 22.469\n",
      "  timestamp: 1627310658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         22791.1</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">0.00270482</td><td style=\"text-align: right;\">            0.391103</td><td style=\"text-align: right;\">           -0.379262</td><td style=\"text-align: right;\">            167.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 242607\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-50-40\n",
      "  done: false\n",
      "  episode_len_mean: 168.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.33161893486976624\n",
      "  episode_reward_mean: 3.018677234649658e-05\n",
      "  episode_reward_min: -0.3792615979909897\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1465\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6880155950784683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014036066248081625\n",
      "          policy_loss: 0.02419271832332015\n",
      "          total_loss: 0.10377682454418391\n",
      "          vf_explained_var: 0.524625301361084\n",
      "          vf_loss: 0.06537259463220835\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6726411581039429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011684330529533327\n",
      "          policy_loss: 0.016035443579312414\n",
      "          total_loss: 0.08060176821891218\n",
      "          vf_explained_var: 0.7148057818412781\n",
      "          vf_loss: 0.05273593938909471\n",
      "    num_agent_steps_sampled: 242607\n",
      "    num_agent_steps_trained: 242607\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.451115241635687\n",
      "    ram_util_percent: 62.57546468401488\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.394713908433914\n",
      "    player_1: 1.9473464787006378\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.004919863641262055\n",
      "    player_1: -0.0048896768689155575\n",
      "  policy_reward_min:\n",
      "    player_0: -1.9634275138378143\n",
      "    player_1: -2.42927148938179\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07795062557185631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9714395536215852\n",
      "    mean_inference_ms: 4.099987808211278\n",
      "    mean_raw_obs_processing_ms: 0.24261354807853105\n",
      "  time_since_restore: 23172.989816188812\n",
      "  time_this_iter_s: 381.927862405777\n",
      "  time_total_s: 23172.989816188812\n",
      "  timers:\n",
      "    learn_throughput: 10.634\n",
      "    learn_time_ms: 376152.406\n",
      "    sample_throughput: 882.89\n",
      "    sample_time_ms: 4530.576\n",
      "    update_time_ms: 23.556\n",
      "  timestamp: 1627311040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">           23173</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">3.01868e-05</td><td style=\"text-align: right;\">            0.331619</td><td style=\"text-align: right;\">           -0.379262</td><td style=\"text-align: right;\">            168.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 246583\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_07-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 168.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.33161893486976624\n",
      "  episode_reward_mean: 9.263306856155396e-05\n",
      "  episode_reward_min: -0.3792615979909897\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1489\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7474690601229668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011552578362170607\n",
      "          policy_loss: 0.027831846848130226\n",
      "          total_loss: 0.09721398772671819\n",
      "          vf_explained_var: 0.6995270252227783\n",
      "          vf_loss: 0.05768515937961638\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7407842874526978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015308494679629803\n",
      "          policy_loss: 0.0215961541980505\n",
      "          total_loss: 0.09651830943766981\n",
      "          vf_explained_var: 0.7553610801696777\n",
      "          vf_loss: 0.059422307880595326\n",
      "    num_agent_steps_sampled: 246583\n",
      "    num_agent_steps_trained: 246583\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.301682242990653\n",
      "    ram_util_percent: 63.66635514018693\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.3095973432064056\n",
      "    player_1: 2.6171310245990753\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.005606762170791626\n",
      "    player_1: 0.00569939523935318\n",
      "  policy_reward_min:\n",
      "    player_0: -2.633804678916931\n",
      "    player_1: -2.336764395236969\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07779161080175964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9721044990487754\n",
      "    mean_inference_ms: 4.100255160086237\n",
      "    mean_raw_obs_processing_ms: 0.242591086974832\n",
      "  time_since_restore: 23553.731684207916\n",
      "  time_this_iter_s: 380.741868019104\n",
      "  time_total_s: 23553.731684207916\n",
      "  timers:\n",
      "    learn_throughput: 10.64\n",
      "    learn_time_ms: 375927.872\n",
      "    sample_throughput: 882.465\n",
      "    sample_time_ms: 4532.759\n",
      "    update_time_ms: 22.165\n",
      "  timestamp: 1627311421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         23553.7</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">9.26331e-05</td><td style=\"text-align: right;\">            0.331619</td><td style=\"text-align: right;\">           -0.379262</td><td style=\"text-align: right;\">            168.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=13488)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 250562\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 166.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.33161893486976624\n",
      "  episode_reward_mean: 0.0005428910255432129\n",
      "  episode_reward_min: -0.3792615979909897\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1514\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6843242086470127\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013267039554193616\n",
      "          policy_loss: 0.024193664983613417\n",
      "          total_loss: 0.12667646817862988\n",
      "          vf_explained_var: 0.733250617980957\n",
      "          vf_loss: 0.08904991997405887\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7628333270549774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01168178697116673\n",
      "          policy_loss: 0.01087075300165452\n",
      "          total_loss: 0.13795167044736445\n",
      "          vf_explained_var: 0.707629919052124\n",
      "          vf_loss: 0.11525310296565294\n",
      "    num_agent_steps_sampled: 250562\n",
      "    num_agent_steps_trained: 250562\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.377449168207024\n",
      "    ram_util_percent: 64.03160813308689\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 2.3095973432064056\n",
      "    player_1: 2.6413649916648865\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.006621309220790863\n",
      "    player_1: -0.00607841819524765\n",
      "  policy_reward_min:\n",
      "    player_0: -2.848096251487732\n",
      "    player_1: -2.336764395236969\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07779592176894748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9730369465784519\n",
      "    mean_inference_ms: 4.100151392058652\n",
      "    mean_raw_obs_processing_ms: 0.24266062410718434\n",
      "  time_since_restore: 23937.53644132614\n",
      "  time_this_iter_s: 383.8047571182251\n",
      "  time_total_s: 23937.53644132614\n",
      "  timers:\n",
      "    learn_throughput: 10.621\n",
      "    learn_time_ms: 376609.895\n",
      "    sample_throughput: 881.595\n",
      "    sample_time_ms: 4537.232\n",
      "    update_time_ms: 21.914\n",
      "  timestamp: 1627311805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         23937.5</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">0.000542891</td><td style=\"text-align: right;\">            0.331619</td><td style=\"text-align: right;\">           -0.379262</td><td style=\"text-align: right;\">            166.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=9864)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 254540\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 167.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.32684817910194397\n",
      "  episode_reward_mean: 0.00044394657015800476\n",
      "  episode_reward_min: -0.31120985746383667\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1537\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6626420132815838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010949589777737856\n",
      "          policy_loss: 0.013309201807714999\n",
      "          total_loss: 0.11881820141570643\n",
      "          vf_explained_var: 0.6997231245040894\n",
      "          vf_loss: 0.09442254295572639\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7255198769271374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013139789225533605\n",
      "          policy_loss: 0.018027929007075727\n",
      "          total_loss: 0.1395357302390039\n",
      "          vf_explained_var: 0.6860789060592651\n",
      "          vf_loss: 0.10820376686751842\n",
      "    num_agent_steps_sampled: 254540\n",
      "    num_agent_steps_trained: 254540\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.590018484288356\n",
      "    ram_util_percent: 62.57800369685768\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.240053206682205\n",
      "    player_1: 2.6413649916648865\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.033046176135540006\n",
      "    player_1: 0.03349012270569801\n",
      "  policy_reward_min:\n",
      "    player_0: -2.848096251487732\n",
      "    player_1: -3.2901869527995586\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0779497930245187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9733040267364601\n",
      "    mean_inference_ms: 4.101524019583994\n",
      "    mean_raw_obs_processing_ms: 0.2427308974174494\n",
      "  time_since_restore: 24321.923338890076\n",
      "  time_this_iter_s: 384.3868975639343\n",
      "  time_total_s: 24321.923338890076\n",
      "  timers:\n",
      "    learn_throughput: 10.614\n",
      "    learn_time_ms: 376843.22\n",
      "    sample_throughput: 878.837\n",
      "    sample_time_ms: 4551.471\n",
      "    update_time_ms: 22.979\n",
      "  timestamp: 1627312189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         24321.9</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">0.000443947</td><td style=\"text-align: right;\">            0.326848</td><td style=\"text-align: right;\">            -0.31121</td><td style=\"text-align: right;\">             167.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 258516\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 167.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.32684817910194397\n",
      "  episode_reward_mean: -0.0007197976112365723\n",
      "  episode_reward_min: -0.31120985746383667\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1561\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.652814943343401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01309731270885095\n",
      "          policy_loss: 0.007236378325615078\n",
      "          total_loss: 0.07453252421692014\n",
      "          vf_explained_var: 0.7244385480880737\n",
      "          vf_loss: 0.054035119945183396\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.667686078697443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010541205760091543\n",
      "          policy_loss: 0.011722630908479914\n",
      "          total_loss: 0.08437021874124184\n",
      "          vf_explained_var: 0.7284851670265198\n",
      "          vf_loss: 0.06197461928240955\n",
      "    num_agent_steps_sampled: 258516\n",
      "    num_agent_steps_trained: 258516\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.461992619926198\n",
      "    ram_util_percent: 64.08819188191883\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.240053206682205\n",
      "    player_1: 2.8613582998514175\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.04529665172100067\n",
      "    player_1: 0.0445768541097641\n",
      "  policy_reward_min:\n",
      "    player_0: -2.8682026267051697\n",
      "    player_1: -3.2901869527995586\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07810247245260933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9735708117509938\n",
      "    mean_inference_ms: 4.101256786401408\n",
      "    mean_raw_obs_processing_ms: 0.24279537025801973\n",
      "  time_since_restore: 24706.723376750946\n",
      "  time_this_iter_s: 384.80003786087036\n",
      "  time_total_s: 24706.723376750946\n",
      "  timers:\n",
      "    learn_throughput: 10.598\n",
      "    learn_time_ms: 377416.179\n",
      "    sample_throughput: 880.848\n",
      "    sample_time_ms: 4541.081\n",
      "    update_time_ms: 23.448\n",
      "  timestamp: 1627312574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         24706.7</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">-0.000719798</td><td style=\"text-align: right;\">            0.326848</td><td style=\"text-align: right;\">            -0.31121</td><td style=\"text-align: right;\">             167.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 262492\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-22-38\n",
      "  done: false\n",
      "  episode_len_mean: 166.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.32684817910194397\n",
      "  episode_reward_mean: 0.00033213049173355105\n",
      "  episode_reward_min: -0.31120985746383667\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1586\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.685294859111309\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010729820729466155\n",
      "          policy_loss: 0.007895790389738977\n",
      "          total_loss: 0.13219217507867143\n",
      "          vf_explained_var: 0.6894916892051697\n",
      "          vf_loss: 0.11343244044110179\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7131962403655052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009682103496743366\n",
      "          policy_loss: 0.01681116793770343\n",
      "          total_loss: 0.13321499899029732\n",
      "          vf_explained_var: 0.6774176359176636\n",
      "          vf_loss: 0.10660069901496172\n",
      "    num_agent_steps_sampled: 262492\n",
      "    num_agent_steps_trained: 262492\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.629759704251388\n",
      "    ram_util_percent: 61.64325323475046\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.240053206682205\n",
      "    player_1: 2.8613582998514175\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0524788436293602\n",
      "    player_1: 0.05281097412109375\n",
      "  policy_reward_min:\n",
      "    player_0: -2.8682026267051697\n",
      "    player_1: -3.2901869527995586\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07843120034323631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.973604015154506\n",
      "    mean_inference_ms: 4.102028975091176\n",
      "    mean_raw_obs_processing_ms: 0.2427212260432453\n",
      "  time_since_restore: 25090.902411699295\n",
      "  time_this_iter_s: 384.179034948349\n",
      "  time_total_s: 25090.902411699295\n",
      "  timers:\n",
      "    learn_throughput: 10.589\n",
      "    learn_time_ms: 377760.838\n",
      "    sample_throughput: 880.453\n",
      "    sample_time_ms: 4543.114\n",
      "    update_time_ms: 22.132\n",
      "  timestamp: 1627312958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         25090.9</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">0.00033213</td><td style=\"text-align: right;\">            0.326848</td><td style=\"text-align: right;\">            -0.31121</td><td style=\"text-align: right;\">            166.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 266470\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 169.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.3100321814417839\n",
      "  episode_reward_mean: -0.0006485983729362487\n",
      "  episode_reward_min: -0.2879130467772484\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1608\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6748069562017918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012365894392132759\n",
      "          policy_loss: 0.0016048342222347856\n",
      "          total_loss: 0.10891928721684963\n",
      "          vf_explained_var: 0.5916390419006348\n",
      "          vf_loss: 0.09479398652911186\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6344286315143108\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012610210687853396\n",
      "          policy_loss: -0.002330566057935357\n",
      "          total_loss: 0.09754144196631387\n",
      "          vf_explained_var: 0.6753605604171753\n",
      "          vf_loss: 0.0871041682548821\n",
      "    num_agent_steps_sampled: 266470\n",
      "    num_agent_steps_trained: 266470\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.65733082706767\n",
      "    ram_util_percent: 64.25187969924811\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.240053206682205\n",
      "    player_1: 2.8613582998514175\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.006582852005958557\n",
      "    player_1: -0.007231450378894806\n",
      "  policy_reward_min:\n",
      "    player_0: -2.8682026267051697\n",
      "    player_1: -3.2901869527995586\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07868648854405945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9726479861040227\n",
      "    mean_inference_ms: 4.102788192177533\n",
      "    mean_raw_obs_processing_ms: 0.24283884479845405\n",
      "  time_since_restore: 25468.594255924225\n",
      "  time_this_iter_s: 377.6918442249298\n",
      "  time_total_s: 25468.594255924225\n",
      "  timers:\n",
      "    learn_throughput: 10.594\n",
      "    learn_time_ms: 377554.886\n",
      "    sample_throughput: 880.742\n",
      "    sample_time_ms: 4541.625\n",
      "    update_time_ms: 21.934\n",
      "  timestamp: 1627313336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         25468.6</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-0.000648598</td><td style=\"text-align: right;\">            0.310032</td><td style=\"text-align: right;\">           -0.287913</td><td style=\"text-align: right;\">            169.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 270446\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-35-18\n",
      "  done: false\n",
      "  episode_len_mean: 168.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9479574114084244\n",
      "  episode_reward_mean: -0.0024201452732086183\n",
      "  episode_reward_min: -0.9518609195947647\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1633\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.654979694634676\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010045646544313058\n",
      "          policy_loss: 0.00812797254184261\n",
      "          total_loss: 0.10282703035045415\n",
      "          vf_explained_var: 0.6926887631416321\n",
      "          vf_loss: 0.08452783850952983\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7263791784644127\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011161384347360581\n",
      "          policy_loss: 0.02764916163869202\n",
      "          total_loss: 0.12314784852787852\n",
      "          vf_explained_var: 0.6807072758674622\n",
      "          vf_loss: 0.08419778477400541\n",
      "    num_agent_steps_sampled: 270446\n",
      "    num_agent_steps_trained: 270446\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.814498141263938\n",
      "    ram_util_percent: 63.12137546468401\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.240053206682205\n",
      "    player_1: 2.8613582998514175\n",
      "  policy_reward_mean:\n",
      "    player_0: -0.0006145492196083068\n",
      "    player_1: -0.0018055960536003113\n",
      "  policy_reward_min:\n",
      "    player_0: -2.8682026267051697\n",
      "    player_1: -3.2901869527995586\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07867827199850964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9719755837192801\n",
      "    mean_inference_ms: 4.102938004581352\n",
      "    mean_raw_obs_processing_ms: 0.24257904005077424\n",
      "  time_since_restore: 25850.52002263069\n",
      "  time_this_iter_s: 381.9257667064667\n",
      "  time_total_s: 25850.52002263069\n",
      "  timers:\n",
      "    learn_throughput: 10.595\n",
      "    learn_time_ms: 377535.523\n",
      "    sample_throughput: 880.348\n",
      "    sample_time_ms: 4543.655\n",
      "    update_time_ms: 21.003\n",
      "  timestamp: 1627313718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         25850.5</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-0.00242015</td><td style=\"text-align: right;\">            0.947957</td><td style=\"text-align: right;\">           -0.951861</td><td style=\"text-align: right;\">            168.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=17992)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 274424\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 167.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9479574114084244\n",
      "  episode_reward_mean: -0.0002464443445205688\n",
      "  episode_reward_min: -0.9518609195947647\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1657\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.7164765745401382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01360721339005977\n",
      "          policy_loss: 0.006629800016526133\n",
      "          total_loss: 0.11293217819184065\n",
      "          vf_explained_var: 0.6374211311340332\n",
      "          vf_loss: 0.09252507239580154\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6820016391575336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010922142217168584\n",
      "          policy_loss: 0.011633597401669249\n",
      "          total_loss: 0.12149668531492352\n",
      "          vf_explained_var: 0.7491077184677124\n",
      "          vf_loss: 0.09880442079156637\n",
      "    num_agent_steps_sampled: 274424\n",
      "    num_agent_steps_trained: 274424\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.710800744878956\n",
      "    ram_util_percent: 63.80055865921786\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.435867488384247\n",
      "    player_1: 2.49336339533329\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.06390171021223068\n",
      "    player_1: -0.06414815455675126\n",
      "  policy_reward_min:\n",
      "    player_0: -2.4506324529647827\n",
      "    player_1: -3.448827847838402\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07862205308926967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9718652831794847\n",
      "    mean_inference_ms: 4.103274748817835\n",
      "    mean_raw_obs_processing_ms: 0.24274124637476036\n",
      "  time_since_restore: 26231.877499341965\n",
      "  time_this_iter_s: 381.3574767112732\n",
      "  time_total_s: 26231.877499341965\n",
      "  timers:\n",
      "    learn_throughput: 10.601\n",
      "    learn_time_ms: 377320.893\n",
      "    sample_throughput: 878.907\n",
      "    sample_time_ms: 4551.107\n",
      "    update_time_ms: 21.077\n",
      "  timestamp: 1627314100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         26231.9</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">-0.000246444</td><td style=\"text-align: right;\">            0.947957</td><td style=\"text-align: right;\">           -0.951861</td><td style=\"text-align: right;\">            167.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 278401\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 167.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9479574114084244\n",
      "  episode_reward_mean: 0.00012546777725219727\n",
      "  episode_reward_min: -0.9518609195947647\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1680\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6740572713315487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012843323842389509\n",
      "          policy_loss: 0.006223243137355894\n",
      "          total_loss: 0.09761864342726767\n",
      "          vf_explained_var: 0.3442728817462921\n",
      "          vf_loss: 0.07839153753593564\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.673831894993782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010673487558960915\n",
      "          policy_loss: 0.007280822639586404\n",
      "          total_loss: 0.0961076058447361\n",
      "          vf_explained_var: 0.4472145736217499\n",
      "          vf_loss: 0.07801987370476127\n",
      "    num_agent_steps_sampled: 278401\n",
      "    num_agent_steps_trained: 278401\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.145018450184503\n",
      "    ram_util_percent: 64.28745387453874\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.435867488384247\n",
      "    player_1: 2.49336339533329\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.026868749856948853\n",
      "    player_1: -0.026743282079696656\n",
      "  policy_reward_min:\n",
      "    player_0: -2.4506324529647827\n",
      "    player_1: -3.448827847838402\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07851853098345819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9716870804469468\n",
      "    mean_inference_ms: 4.103746194592636\n",
      "    mean_raw_obs_processing_ms: 0.24281629245546263\n",
      "  time_since_restore: 26616.651341676712\n",
      "  time_this_iter_s: 384.7738423347473\n",
      "  time_total_s: 26616.651341676712\n",
      "  timers:\n",
      "    learn_throughput: 10.583\n",
      "    learn_time_ms: 377953.235\n",
      "    sample_throughput: 881.65\n",
      "    sample_time_ms: 4536.947\n",
      "    update_time_ms: 21.175\n",
      "  timestamp: 1627314484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         26616.7</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">0.000125468</td><td style=\"text-align: right;\">            0.947957</td><td style=\"text-align: right;\">           -0.951861</td><td style=\"text-align: right;\">            167.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=15516)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 282377\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_08-54-33\n",
      "  done: false\n",
      "  episode_len_mean: 168.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9479574114084244\n",
      "  episode_reward_mean: 0.0010836802423000336\n",
      "  episode_reward_min: -0.9518609195947647\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1705\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6719315238296986\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011900842015165836\n",
      "          policy_loss: 0.0005236782599240541\n",
      "          total_loss: 0.08530562371015549\n",
      "          vf_explained_var: 0.529930830001831\n",
      "          vf_loss: 0.07273234357126057\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6486050821840763\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008999196579679847\n",
      "          policy_loss: 0.019426930899498984\n",
      "          total_loss: 0.13517300924286246\n",
      "          vf_explained_var: 0.5231250524520874\n",
      "          vf_loss: 0.10663438402116299\n",
      "    num_agent_steps_sampled: 282377\n",
      "    num_agent_steps_trained: 282377\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.010401459854014\n",
      "    ram_util_percent: 64.04051094890512\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.435867488384247\n",
      "    player_1: 2.49336339533329\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.02125113844871521\n",
      "    player_1: -0.020167458206415176\n",
      "  policy_reward_min:\n",
      "    player_0: -2.4506324529647827\n",
      "    player_1: -3.448827847838402\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07838482041215523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9717438187066882\n",
      "    mean_inference_ms: 4.104872435691644\n",
      "    mean_raw_obs_processing_ms: 0.24269548749321138\n",
      "  time_since_restore: 27005.287352323532\n",
      "  time_this_iter_s: 388.63601064682007\n",
      "  time_total_s: 27005.287352323532\n",
      "  timers:\n",
      "    learn_throughput: 10.565\n",
      "    learn_time_ms: 378618.657\n",
      "    sample_throughput: 880.709\n",
      "    sample_time_ms: 4541.794\n",
      "    update_time_ms: 20.723\n",
      "  timestamp: 1627314873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         27005.3</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">0.00108368</td><td style=\"text-align: right;\">            0.947957</td><td style=\"text-align: right;\">           -0.951861</td><td style=\"text-align: right;\">             168.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 286355\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_09-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 168.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5030038952827454\n",
      "  episode_reward_mean: 0.0026683688163757326\n",
      "  episode_reward_min: -0.5076117813587189\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1727\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6827193647623062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010703535983338952\n",
      "          policy_loss: -0.009723388706333935\n",
      "          total_loss: 0.09671742626233026\n",
      "          vf_explained_var: 0.6246081590652466\n",
      "          vf_loss: 0.09560347837395966\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6017962358891964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011513186094816774\n",
      "          policy_loss: 0.004696671850979328\n",
      "          total_loss: 0.09251530980691314\n",
      "          vf_explained_var: 0.7230772972106934\n",
      "          vf_loss: 0.07616153755225241\n",
      "    num_agent_steps_sampled: 286355\n",
      "    num_agent_steps_trained: 286355\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.872007366482505\n",
      "    ram_util_percent: 60.40810313075506\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.435867488384247\n",
      "    player_1: 2.0427705347537994\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.04258096247911453\n",
      "    player_1: -0.0399125936627388\n",
      "  policy_reward_min:\n",
      "    player_0: -2.281598225235939\n",
      "    player_1: -3.448827847838402\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07838858382483281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9714013861606151\n",
      "    mean_inference_ms: 4.105478209952285\n",
      "    mean_raw_obs_processing_ms: 0.24301774703950785\n",
      "  time_since_restore: 27390.636364221573\n",
      "  time_this_iter_s: 385.34901189804077\n",
      "  time_total_s: 27390.636364221573\n",
      "  timers:\n",
      "    learn_throughput: 10.552\n",
      "    learn_time_ms: 379074.568\n",
      "    sample_throughput: 879.575\n",
      "    sample_time_ms: 4547.653\n",
      "    update_time_ms: 21.11\n",
      "  timestamp: 1627315258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be, 0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         27390.6</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">0.00266837</td><td style=\"text-align: right;\">            0.503004</td><td style=\"text-align: right;\">           -0.507612</td><td style=\"text-align: right;\">            168.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\collectors\\simple_list_collector.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m C:\\Users\\408aa\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\sample_batch.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\u001b[2m\u001b[36m(pid=16724)\u001b[0m   self[k] = np.array(v)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_ChessEnv_d8249_00000:\n",
      "  agent_timesteps_total: 290331\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_09-07-27\n",
      "  done: false\n",
      "  episode_len_mean: 168.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5030038952827454\n",
      "  episode_reward_mean: -0.0016481637954711913\n",
      "  episode_reward_min: -0.5076117813587189\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1752\n",
      "  experiment_id: 0bb2662462f74b3ab6e6a400b1419099\n",
      "  hostname: Asus-ZephryusG14\n",
      "  info:\n",
      "    learner:\n",
      "      player_0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.5815132036805153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011957329872529954\n",
      "          policy_loss: 0.007326827268116176\n",
      "          total_loss: 0.06937311892397702\n",
      "          vf_explained_var: 0.6833558082580566\n",
      "          vf_loss: 0.04993949248455465\n",
      "      player_1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125000000000002\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6184342838823795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009683648066129535\n",
      "          policy_loss: 0.004577190964482725\n",
      "          total_loss: 0.07092969189397991\n",
      "          vf_explained_var: 0.7178927659988403\n",
      "          vf_loss: 0.056547805201262236\n",
      "    num_agent_steps_sampled: 290331\n",
      "    num_agent_steps_trained: 290331\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 10.0.0.37\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.925912408759125\n",
      "    ram_util_percent: 58.40766423357664\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    player_0: 3.435867488384247\n",
      "    player_1: 1.9925835728645325\n",
      "  policy_reward_mean:\n",
      "    player_0: 0.03069391280412674\n",
      "    player_1: -0.03234207659959793\n",
      "  policy_reward_min:\n",
      "    player_0: -1.9680942595005035\n",
      "    player_1: -3.448827847838402\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07837778341789065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9703853195883195\n",
      "    mean_inference_ms: 4.107834951586187\n",
      "    mean_raw_obs_processing_ms: 0.2427629707373219\n",
      "  time_since_restore: 27779.414870500565\n",
      "  time_this_iter_s: 388.7785062789917\n",
      "  time_total_s: 27779.414870500565\n",
      "  timers:\n",
      "    learn_throughput: 10.538\n",
      "    learn_time_ms: 379567.887\n",
      "    sample_throughput: 878.748\n",
      "    sample_time_ms: 4551.931\n",
      "    update_time_ms: 21.637\n",
      "  timestamp: 1627315647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: d8249_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14.0/16 CPUs, 0/1 GPUs, 0.0/5.05 GiB heap, 0.0/2.52 GiB objects (0.0/4.0 CPU_group_0_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_3_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_1_d455c44098ecb0c1df2e63374acc77be, 0.0/14.0 CPU_group_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_4_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_2_d455c44098ecb0c1df2e63374acc77be, 0.0/2.0 CPU_group_5_d455c44098ecb0c1df2e63374acc77be)<br>Result logdir: c:\\Users\\408aa\\Desktop\\Code\\Python\\Chess_AI\\Chess_Policy<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ChessEnv_d8249_00000</td><td>RUNNING </td><td>10.0.0.37:5700</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         27779.4</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">-0.00164816</td><td style=\"text-align: right;\">            0.503004</td><td style=\"text-align: right;\">           -0.507612</td><td style=\"text-align: right;\">            168.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-26 09:10:00,211\tWARNING tune.py:507 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rl_env': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "c7f31768882b879b5bb7313a12d7fccae5219ad918800fa4a488d70b158d3c25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}